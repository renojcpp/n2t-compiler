package jack_parser

import (
	"errors"
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	writer io.Writer
	index  int
	err    error
}

func NewParser(tokens []jack_tokenizer.Token, w io.Writer) *parser {
	return &parser{
		tokens,
		w,
		0,
		nil,
	}
}

func (s *parser) process(pairs []tokenpair) {
	if s.matches(pairs) {
		io.WriteString(s.writer, (s.Current().Lexeme))
	} else {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
	}
	s.Advance()
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)
	s.process(tp)
}

func (s *parser) helper_typeVarName() {
	s.process(typePair)
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) wrap(tag string, f func()) {
	io.WriteString(s.writer, "<"+tag+">")
	f()
	io.WriteString(s.writer, "</"+tag+">")
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<symbol>")
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
	io.WriteString(s.writer, "</symbol>")
}

func (s *parser) identifierHelper() {
	io.WriteString(s.writer, "<identifier>")
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
	io.WriteString(s.writer, "</identifier>")
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<keyword>")
	s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
	io.WriteString(s.writer, "</keyword>")
}

// compiles a Class
func (s *parser) Class() {
	io.WriteString(s.writer, "<class>")

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</class>")
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	io.WriteString(s.writer, "<classVarDec>")

	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
		})
	})

	s.helper_typeVarName()
	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</classVarDec>")
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	io.WriteString(s.writer, "<subroutineDec>")
	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		})
	})

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	s.identifierHelper()

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Parameters()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
	io.WriteString(s.writer, "</subroutineDec>")
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() {
	processTypeVarName := func() {
		s.process(typePair)
		s.identifierHelper()
	}
	io.WriteString(s.writer, "<parameters>")
	if s.matches(typePair) {

		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}
	io.WriteString(s.writer, "</parameters>")
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	io.WriteString(s.writer, "<subroutineBody>")
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}

	s.Statements()

	io.WriteString(s.writer, "</subroutineBody>")
}

// Compiles a var declaration
func (s *parser) VarDec() {
	io.WriteString(s.writer, "<varDec>")
	s.keywordHelper(jack_tokenizer.KW_VAR)

	s.helper_typeVarName()

	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</varDec>")
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		io.WriteString(s.writer, "<statements>")
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
		io.WriteString(s.writer, "</statements>")
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	io.WriteString(s.writer, "<letStatement>")
	s.keywordHelper(jack_tokenizer.KW_LET)
	s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</letStatement>")
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	io.WriteString(s.writer, "<ifStatement>")

	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {

		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	io.WriteString(s.writer, "</ifStatement>")
}

// Compiles a While statement
func (s *parser) While() {
	io.WriteString(s.writer, "<whileStatement>")
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</whileStatement>")
}

// Compiles a Do statement
func (s *parser) Do() {
	io.WriteString(s.writer, "<doStatement>")
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.SubroutineCall()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</doStatement>")
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	io.WriteString(s.writer, "<returnStatement>")
	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</returnStatement>")
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	io.WriteString(s.writer, "<expression>")
	s.Term()
	for s.matches(op) {
		s.process(op)
		s.Term()
	}
	io.WriteString(s.writer, "</expression>")
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	io.WriteString(s.writer, "<term>")
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				s.identifierHelper()
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				s.identifierHelper()
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.STRING_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_TRUE:
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
		case jack_tokenizer.KW_THIS:
			s.keywordHelper(s.Current().Subtype)
		}
	}

	io.WriteString(s.writer, "</term>")
}

func (s *parser) SubroutineCall() {
	io.WriteString(s.writer, "<subroutineCall>")
	s.identifierHelper()

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		s.identifierHelper()
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	}
	io.WriteString(s.writer, "</subroutineCall>")
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	io.WriteString(s.writer, "<expressionList>")
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}
	io.WriteString(s.writer, "</expressionList>")

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y
func ParseGrammar(tokens []jack_tokenizer.Token) func(io.Writer) error {
	return func(w io.Writer) error {
		parser := NewParser(tokens, w)

		err := parser.Parse()
		return err
	}
}
package jack_parser

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestParseGrammar(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := ParseGrammar(tokens)
					fmt.Println("outputting")
					err = toOut(os.Stdout)

					if err != nil {
						t.Errorf("%s", err.Error())
					}
				}
			}
		})
	}
}
package jack_tokenizer

type Token struct {
	Lexeme    string
	Line      int
	Tokentype TokenType
	Subtype   TokenSubtype
}

func NewToken(lexeme string, line int, tokentype TokenType, subtype TokenSubtype) Token {
	return Token{
		lexeme,
		line,
		tokentype,
		subtype,
	}
}

type TokenType int

const (
	ERROR = -1
)

//go:generate string -type=TokenType
const (
	KEYWORD TokenType = iota
	SYMBOL
	INT_CONSTANT
	STRING_CONSTANT
	IDENTIFIER
)

type TokenSubtype int

//go:generate stringer -type=TokenSubtype
const (
	UNKNOWN TokenSubtype = -1
	NONE    TokenSubtype = iota
	// Keywords
	KW_CLASS
	KW_CONSTRUCTOR
	KW_FUNCTION
	KW_METHOD
	KW_FIELD
	KW_STATIC
	KW_VAR
	KW_INT
	KW_CHAR
	KW_BOOLEAN
	KW_VOID
	KW_TRUE
	KW_FALSE
	KW_NULL
	KW_THIS
	KW_LET
	KW_DO
	KW_IF
	KW_ELSE
	KW_WHILE
	KW_RETURN

	// symbols
	SYM_LEFT_BRACE
	SYM_RIGHT_BRACE
	SYM_LEFT_PAREN
	SYM_RIGHT_PAREN
	SYM_LEFT_BRACK
	SYM_RIGHT_BRACK
	SYM_PERIOD
	SYM_COMMA
	SYM_SEMICOLON
	SYM_PLUS
	SYM_MINUS
	SYM_ASTERISK
	SYM_SLASH
	SYM_AMPERSAND
	SYM_PIPE
	SYM_LESS_THAN
	SYM_GREATER_THAN
	SYM_EQUALS
	SYM_TILDE
)

type tokenpair struct {
	tt TokenType
	st TokenSubtype
}

type luxmap map[string]tokenpair

var mp = luxmap{
	"class":       {KEYWORD, KW_CLASS},
	"constructor": {KEYWORD, KW_CONSTRUCTOR},
	"function":    {KEYWORD, KW_FUNCTION},
	"method":      {KEYWORD, KW_METHOD},
	"field":       {KEYWORD, KW_FIELD},
	"static":      {KEYWORD, KW_STATIC},
	"var":         {KEYWORD, KW_VAR},
	"int":         {KEYWORD, KW_INT},
	"char":        {KEYWORD, KW_CHAR},
	"boolean":     {KEYWORD, KW_BOOLEAN},
	"void":        {KEYWORD, KW_VOID},
	"true":        {KEYWORD, KW_TRUE},
	"false":       {KEYWORD, KW_FALSE},
	"null":        {KEYWORD, KW_NULL},
	"this":        {KEYWORD, KW_THIS},
	"let":         {KEYWORD, KW_LET},
	"do":          {KEYWORD, KW_DO},
	"if":          {KEYWORD, KW_IF},
	"else":        {KEYWORD, KW_ELSE},
	"while":       {KEYWORD, KW_WHILE},
	"return":      {KEYWORD, KW_RETURN},

	"{": {SYMBOL, SYM_LEFT_BRACE},
	"}": {SYMBOL, SYM_RIGHT_BRACE},
	"(": {SYMBOL, SYM_LEFT_PAREN},
	")": {SYMBOL, SYM_RIGHT_PAREN},
	"[": {SYMBOL, SYM_LEFT_BRACK},
	"]": {SYMBOL, SYM_RIGHT_BRACK},
	".": {SYMBOL, SYM_PERIOD},
	",": {SYMBOL, SYM_COMMA},
	";": {SYMBOL, SYM_SEMICOLON},
	"+": {SYMBOL, SYM_PLUS},
	"-": {SYMBOL, SYM_MINUS},
	"*": {SYMBOL, SYM_ASTERISK},
	"/": {SYMBOL, SYM_SLASH},
	"&": {SYMBOL, SYM_AMPERSAND},
	"|": {SYMBOL, SYM_PIPE},
	"<": {SYMBOL, SYM_LESS_THAN},
	">": {SYMBOL, SYM_GREATER_THAN},
	"=": {SYMBOL, SYM_EQUALS},
	"~": {SYMBOL, SYM_TILDE},
}
package jack_tokenizer

import (
	"errors"
	"io"
	"strconv"
	"strings"
)

type scanner struct {
	bytes []byte
	index int
}

func (s *scanner) atEnd() bool {
	return s.index >= len(s.bytes)
}

func (s *scanner) advance() {
	s.index++
}

func (s *scanner) peek() (byte, error) {
	if s.atEnd() || s.index+1 >= len(s.bytes) {
		return 0, errors.New("peek beyond")
	}

	return s.bytes[s.index+1], nil
}

func (s *scanner) current() byte {
	return s.bytes[s.index]
}

func NewScanner(b []byte) scanner {
	return scanner{b, 0}
}

func isNumber(b byte) bool {
	_, err := strconv.ParseInt(string(b), 10, 8)
	return err == nil
}

func isLetter(c byte) bool {
	return (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')
}

func Tokenize(r io.Reader) ([]Token, error) {
	chars, err := io.ReadAll(r)
	if err != nil {
		return nil, err
	}
	var parseError error
	parseError = nil
	scan := NewScanner(chars)
	tokens := make([]Token, 0)
	line := 0

	writeInt := func() (string, error) {
		var ss strings.Builder
		for !scan.atEnd() && isNumber(scan.current()) {
			ss.WriteByte(scan.current())
			scan.advance()
		}

		if !scan.atEnd() && isLetter(scan.current()) {
			return "", errors.New("letter found " + string(scan.current()))
		}

		return ss.String(), nil
	}

	for !scan.atEnd() {
		ch := scan.current()
		pair, ok := mp[string(ch)]
		switch {
		// symbols
		case ch == '/':
			peek, err := scan.peek()
			if err != nil {
				parseError = err
			} else {
				if peek == '/' { // single comment
					for !scan.atEnd() && scan.current() != '\n' {
						scan.advance()
					}
				} else if peek == '*' { // multi line comment
					scan.advance()
					peek, _ = scan.peek()
					for !scan.atEnd() && (scan.current() != '*' || peek != '/') {
						scan.advance()
						peek, _ = scan.peek()
					}
					scan.advance()
				} else {
					tokens = append(tokens, NewToken(string(ch), line, SYMBOL, SYM_SLASH))
				}
			}
			scan.advance()
		case ch == '"':
			var ss strings.Builder
			scan.advance()
			for !scan.atEnd() && (scan.current() != '\n' && scan.current() != '"') {
				ss.WriteByte(scan.current())
				scan.advance()
			}

			sres := ss.String()
			if !scan.atEnd() && scan.current() == '"' {
				tokens = append(tokens, NewToken(sres, line, STRING_CONSTANT, NONE))
				scan.advance()
			} else {
				parseError = errors.New("unterminated string")
				tokens = append(tokens, NewToken(sres, line, ERROR, NONE))
			}
		case ok && pair.tt == SYMBOL:
			tokens = append(tokens, NewToken(string(ch), line, pair.tt, pair.st))
			scan.advance()
		case isNumber(ch):
			tt := INT_CONSTANT
			st := NONE
			numberStr, err := writeInt()
			if err != nil {
				tt = ERROR
				parseError = err
			}
			tokens = append(tokens, NewToken(numberStr, line, TokenType(tt), TokenSubtype(st)))

		case isLetter(ch) || ch == '_': // identifier, or keyword
			var ss strings.Builder
			for !scan.atEnd() && isLetter(scan.current()) {
				ss.WriteByte(scan.current())
				scan.advance()
			}
			sres := ss.String()
			pair, ok = mp[sres]
			if ok { // keyword
				tokens = append(tokens, NewToken(sres, line, pair.tt, pair.st))
			} else { // identifier
				tokens = append(tokens, NewToken(sres, line, IDENTIFIER, NONE))
			}
		default: // whitespace or unrecognized
			scan.advance()
		}

		// scan.advance()
		line++
	}

	return tokens, parseError
}
// Code generated by "stringer -type=TokenSubtype"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[UNKNOWN - -1]
	_ = x[NONE-1]
	_ = x[KW_CLASS-2]
	_ = x[KW_CONSTRUCTOR-3]
	_ = x[KW_FUNCTION-4]
	_ = x[KW_METHOD-5]
	_ = x[KW_FIELD-6]
	_ = x[KW_STATIC-7]
	_ = x[KW_VAR-8]
	_ = x[KW_INT-9]
	_ = x[KW_CHAR-10]
	_ = x[KW_BOOLEAN-11]
	_ = x[KW_VOID-12]
	_ = x[KW_TRUE-13]
	_ = x[KW_FALSE-14]
	_ = x[KW_NULL-15]
	_ = x[KW_THIS-16]
	_ = x[KW_LET-17]
	_ = x[KW_DO-18]
	_ = x[KW_IF-19]
	_ = x[KW_ELSE-20]
	_ = x[KW_WHILE-21]
	_ = x[KW_RETURN-22]
	_ = x[SYM_LEFT_BRACE-23]
	_ = x[SYM_RIGHT_BRACE-24]
	_ = x[SYM_LEFT_PAREN-25]
	_ = x[SYM_RIGHT_PAREN-26]
	_ = x[SYM_LEFT_BRACK-27]
	_ = x[SYM_RIGHT_BRACK-28]
	_ = x[SYM_PERIOD-29]
	_ = x[SYM_COMMA-30]
	_ = x[SYM_SEMICOLON-31]
	_ = x[SYM_PLUS-32]
	_ = x[SYM_MINUS-33]
	_ = x[SYM_ASTERISK-34]
	_ = x[SYM_SLASH-35]
	_ = x[SYM_AMPERSAND-36]
	_ = x[SYM_PIPE-37]
	_ = x[SYM_LESS_THAN-38]
	_ = x[SYM_GREATER_THAN-39]
	_ = x[SYM_EQUALS-40]
	_ = x[SYM_TILDE-41]
}

const (
	_TokenSubtype_name_0 = "UNKNOWN"
	_TokenSubtype_name_1 = "NONEKW_CLASSKW_CONSTRUCTORKW_FUNCTIONKW_METHODKW_FIELDKW_STATICKW_VARKW_INTKW_CHARKW_BOOLEANKW_VOIDKW_TRUEKW_FALSEKW_NULLKW_THISKW_LETKW_DOKW_IFKW_ELSEKW_WHILEKW_RETURNSYM_LEFT_BRACESYM_RIGHT_BRACESYM_LEFT_PARENSYM_RIGHT_PARENSYM_LEFT_BRACKSYM_RIGHT_BRACKSYM_PERIODSYM_COMMASYM_SEMICOLONSYM_PLUSSYM_MINUSSYM_ASTERISKSYM_SLASHSYM_AMPERSANDSYM_PIPESYM_LESS_THANSYM_GREATER_THANSYM_EQUALSSYM_TILDE"
)

var (
	_TokenSubtype_index_1 = [...]uint16{0, 4, 12, 26, 37, 46, 54, 63, 69, 75, 82, 92, 99, 106, 114, 121, 128, 134, 139, 144, 151, 159, 168, 182, 197, 211, 226, 240, 255, 265, 274, 287, 295, 304, 316, 325, 338, 346, 359, 375, 385, 394}
)

func (i TokenSubtype) String() string {
	switch {
	case i == -1:
		return _TokenSubtype_name_0
	case 1 <= i && i <= 41:
		i -= 1
		return _TokenSubtype_name_1[_TokenSubtype_index_1[i]:_TokenSubtype_index_1[i+1]]
	default:
		return "TokenSubtype(" + strconv.FormatInt(int64(i), 10) + ")"
	}
}
// Code generated by "stringer -type=TokenType"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[KEYWORD-0]
	_ = x[SYMBOL-1]
	_ = x[INT_CONSTANT-2]
	_ = x[STRING_CONSTANT-3]
	_ = x[IDENTIFIER-4]
}

const _TokenType_name = "KEYWORDSYMBOLINT_CONSTANTSTRING_CONSTANTIDENTIFIER"

var _TokenType_index = [...]uint8{0, 7, 13, 25, 40, 50}

func (i TokenType) String() string {
	if i < 0 || i >= TokenType(len(_TokenType_index)-1) {
		return "TokenType(" + strconv.FormatInt(int64(i), 10) + ")"
	}
	return _TokenType_name[_TokenType_index[i]:_TokenType_index[i+1]]
}
package jack_tokenwriter

import (
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func CreateXMLWriter(tokens []jack_tokenizer.Token) func(io.Writer) {
	var ss strings.Builder

	return func(w io.Writer) {
		for _, token := range tokens {
			tagname, ok := keyword2tag[token.Tokentype]
			if !ok {
				tagname = "unknown"
			}
			escaped, ok := escapeLexeme[token.Lexeme]
			if !ok {
				escaped = token.Lexeme
			}
			ss.WriteString(wrap(tagname, escaped, 1))
		}
		var root strings.Builder
		root.WriteString(wrap("tokens", "\n"+ss.String(), 0))

		io.WriteString(w, root.String())
	}
}

func wrap(tagName, content string, tabs int) string {
	return fmt.Sprintf("%s<%s>%s</%s>\n", strings.Repeat("\t", tabs), tagName, content, tagName)
}

var keyword2tag = map[jack_tokenizer.TokenType]string{
	jack_tokenizer.KEYWORD:         "keyword",
	jack_tokenizer.SYMBOL:          "symbol",
	jack_tokenizer.INT_CONSTANT:    "integerConstant",
	jack_tokenizer.STRING_CONSTANT: "stringConstant",
	jack_tokenizer.IDENTIFIER:      "identifier",
}

var escapeLexeme = map[string]string{
	"<":  "&lt;",
	">":  "&gt;",
	"\"": "&quot;",
	"&":  "&amp;",
}
package jack_tokenwriter

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestCreateXMLWriter(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := CreateXMLWriter(tokens)
					fmt.Println("outputting")
					toOut(os.Stdout)
				}
			}
		})
	}
}
package jack_compiler

import (
	"fmt"
	"io"
)

type VMWriter struct {
	io.WriteCloser
}

var sm = map[SegmentType]string{
	CONSTANT: "constant",
	ARGUMENT: "arg",
	LOCAL:    "local",
	STATIC_S: "static",
	THIS:     "this",
	THAT:     "that",
	POINTER:  "pointer",
	TEMP:     "temp",
}

var am = map[ArithmeticType]string{
	ADD: "add",
	SUB: "sub",
	NEG: "neg",
	EQ:  "eq",
	GT:  "gt",
	LT:  "lt",
	AND: "and",
	OR:  "or",
	NOT: "not",
}

func (s *VMWriter) WritePush(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("push %s %d\n", sm[segment], index))
}

func (s *VMWriter) WritePop(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("pop %s %d\n", sm[segment], index))
}

func (s *VMWriter) WriteArithmetic(arithmetic ArithmeticType) {
	io.WriteString(s, am[arithmetic]+"\n")
}

func (s *VMWriter) WriteLabel(label string) {
	io.WriteString(s, fmt.Sprintf("label %s\n", label))
}

func (s *VMWriter) WriteGoto(label string) {
	io.WriteString(s, fmt.Sprintf("goto %s\n", label))
}

func (s *VMWriter) WriteIf(label string) {
	io.WriteString(s, fmt.Sprintf("if-goto %s\n", label))
}

func (s *VMWriter) WriteCall(name string, nArgs int) {
	io.WriteString(s, fmt.Sprintf("call %s %d\n", name, nArgs))
}

func (s *VMWriter) WriteFunction(name Name, nvars int) {
	io.WriteString(s, fmt.Sprintf("function %s %d\n", name, nvars))
}

func (s *VMWriter) WriteReturn() {
	io.WriteString(s, "return\n")
}

func NewVMWriter(w io.WriteCloser) *VMWriter {
	return &VMWriter{w}
}
package jack_compiler

import jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"

type Name string

type tableentry struct {
	index  int
	typing string
}

type FieldType int

const (
	STATIC_F FieldType = iota
	FIELD
	ARG
	VAR

	NONE
)

var fieldtoSegment = map[FieldType]SegmentType{
	STATIC_F: STATIC_S,
	FIELD:    THIS,
	VAR:      LOCAL,
	ARG:      ARGUMENT,
}

var constructorTTtoFT = map[jack_tokenizer.TokenSubtype]FieldType{
	jack_tokenizer.KW_STATIC: STATIC_F,
	jack_tokenizer.KW_FIELD:  VAR,
}

var subtypeToOp = map[jack_tokenizer.TokenSubtype]ArithmeticType{
	jack_tokenizer.SYM_PLUS:         ADD,
	jack_tokenizer.SYM_MINUS:        SUB,
	jack_tokenizer.SYM_AMPERSAND:    AND,
	jack_tokenizer.SYM_LESS_THAN:    LT,
	jack_tokenizer.SYM_GREATER_THAN: GT,
	jack_tokenizer.SYM_EQUALS:       EQ,
	jack_tokenizer.SYM_PIPE:         OR,
	jack_tokenizer.SYM_TILDE:        NOT,
}

type SymbolTable struct {
	staticTable map[Name]tableentry
	fieldTable  map[Name]tableentry
	argTable    map[Name]tableentry
	varTable    map[Name]tableentry

	staticIndex int
	fieldIndex  int
	argIndex    int
	varIndex    int
}

func (s *SymbolTable) getTable(kind FieldType) (*map[Name]tableentry, *int) {
	switch kind {
	case STATIC_F:
		return &s.staticTable, &s.staticIndex
	case FIELD:
		return &s.fieldTable, &s.fieldIndex
	case ARG:
		return &s.argTable, &s.argIndex
	case VAR:
		return &s.varTable, &s.varIndex
	default:
		panic("unknown segment")
	}
}

func (sym *SymbolTable) find(n Name) *map[Name]tableentry {
	if _, ok := sym.staticTable[n]; ok {
		return &sym.staticTable
	}

	if _, ok := sym.fieldTable[n]; ok {
		return &sym.fieldTable
	}

	if _, ok := sym.argTable[n]; ok {
		return &sym.argTable
	}

	if _, ok := sym.varTable[n]; ok {
		return &sym.varTable
	}

	return nil
}

func (sym *SymbolTable) Reset() {
	sym.staticTable = make(map[Name]tableentry)
	sym.fieldTable = make(map[Name]tableentry)
	sym.argTable = make(map[Name]tableentry)
	sym.varTable = make(map[Name]tableentry)

	sym.staticIndex = 0
	sym.fieldIndex = 0
	sym.argIndex = 0
	sym.varIndex = 0
}

func (sym *SymbolTable) Define(n Name, t string, kind FieldType) {
	table, index := sym.getTable(kind)

	(*table)[n] = tableentry{*index, t}
	*index += 1
}

func (sym *SymbolTable) VarCount(kind FieldType) int {
	table, _ := sym.getTable(kind)

	return len(*table)
}

func (sym *SymbolTable) KindOf(n Name) FieldType {
	if _, ok := sym.staticTable[n]; ok {
		return STATIC_F
	}

	if _, ok := sym.fieldTable[n]; ok {
		return FIELD
	}

	if _, ok := sym.argTable[n]; ok {
		return ARG
	}

	if _, ok := sym.varTable[n]; ok {
		return VAR
	}

	return NONE
}

func (sym *SymbolTable) TypeOf(n Name) string {
	table := sym.find(n)

	return (*table)[n].typing
}

func (sym *SymbolTable) IndexOf(n Name) int {
	table := sym.find(n)

	return (*table)[n].index
}

func NewSymbolTable() *SymbolTable {
	return &SymbolTable{}
}

type SegmentType int

const (
	CONSTANT SegmentType = iota
	ARGUMENT
	LOCAL
	STATIC_S
	THIS
	THAT
	POINTER
	TEMP
)

type ArithmeticType int

const (
	ADD ArithmeticType = iota
	SUB
	NEG
	EQ
	GT
	LT
	AND
	OR
	NOT
)
package jack_compiler

import (
	"errors"
	"fmt"
	"io"
	"strconv"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	index  int

	err          error
	subroutineSt *SymbolTable
	classSt      *SymbolTable
	vmWriter     VMWriter

	className   string
	labelNumber int
}

type symboldata struct {
	name   string
	symbol FieldType
	index  int
}

func NewParser(tokens []jack_tokenizer.Token, vmw VMWriter) *parser {
	return &parser{
		tokens,
		0,
		nil,
		NewSymbolTable(),
		NewSymbolTable(),
		vmw,
		"",
		0,
	}
}

func (s *parser) resolveSymbol(sym string) symboldata {
	res := s.subroutineSt.KindOf(Name(sym))

	if res == NONE {
		res = s.classSt.KindOf(Name(sym))
		if res == NONE {
			return symboldata{
				sym,
				NONE,
				0,
			}
		}

		return symboldata{
			sym,
			res,
			s.classSt.IndexOf(Name(sym)),
		}
	}

	return symboldata{
		sym,
		res,
		s.subroutineSt.IndexOf(Name(sym)),
	}
}

func (s *parser) process(pairs []tokenpair) (*jack_tokenizer.Token, error) {
	var err error
	if !s.matches(pairs) {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
		err = s.err
	}
	ret := s.Current()
	s.Advance()

	return ret, err
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) (*jack_tokenizer.Token, error) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)

	return s.process(tp)
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
}

func (s *parser) identifierHelper() (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
}

// compiles a Class
func (s *parser) Class() {
	s.classSt.Reset()
	s.subroutineSt.Reset()

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	token, err := s.identifierHelper()
	if err == nil {
		s.className = token.Lexeme
	}
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	var ft FieldType
	var typing string
	var varName string
	hasError := false
	token, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	})

	if err == nil {
		ft = constructorTTtoFT[token.Subtype]
	} else {
		hasError = true
	}

	token, err = s.helper_type([]tokenpair{})

	if err == nil {
		typing = token.Lexeme
	} else {
		hasError = true
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	} else {
		hasError = true
	}

	if !hasError {
		s.classSt.Define(Name(varName), typing, ft)
	}

	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
			s.classSt.Define(Name(varName), typing, ft)
		}
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	s.subroutineSt.Reset()

	keyToken, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
	})

	if err == nil && keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.subroutineSt.Define("this", s.className, ARG)
	}

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	token, _ := s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	nargs := s.Parameters()

	s.vmWriter.WriteFunction(Name(fmt.Sprintf("%s.%s", s.className, token.Lexeme)), nargs)

	if keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.vmWriter.WritePush(ARGUMENT, 0)
		s.vmWriter.WritePop(POINTER, 0)
	} else if keyToken.Subtype == jack_tokenizer.KW_CONSTRUCTOR {
		n := s.subroutineSt.VarCount(FIELD)
		s.vmWriter.WritePush(CONSTANT, n)
		s.vmWriter.WriteCall("Memory.alloc", 1)
		s.vmWriter.WritePop(POINTER, 0)
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() int {
	count := 0
	processTypeVarName := func() {
		var typing string
		var varName string
		token, err := s.process(typePair)
		if err == nil {
			typing = token.Lexeme
		}
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
		}

		s.subroutineSt.Define(Name(varName), typing, ARG)
		count++
	}
	if s.matches(typePair) {
		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}

	return count
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}
	s.Statements()
}

// Compiles a var declaration
func (s *parser) VarDec() {
	s.keywordHelper(jack_tokenizer.KW_VAR)
	var typing string
	var varName string

	token, err := s.helper_type(nil)
	if err == nil {
		typing = token.Lexeme
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	}

	s.subroutineSt.Define(Name(varName), typing, FieldType(VAR))
	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	s.keywordHelper(jack_tokenizer.KW_LET)
	token, _ := s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	// pop symbolArgName index

	res := s.resolveSymbol(token.Lexeme)
	s.vmWriter.WritePop(fieldtoSegment[res.symbol], res.index)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	// if-goto label1
	s.vmWriter.WriteIf(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.labelNumber++
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()
	// goto label2
	s.vmWriter.WriteGoto(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {
		// label l1
		s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber-1))
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
}

// Compiles a While statement
func (s *parser) While() {
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	s.labelNumber++
	// if-goto l2
	s.vmWriter.WriteIf(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	// goto l1
	s.vmWriter.WriteGoto(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber-1))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
}

// Compiles a Do statement
func (s *parser) Do() {
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.Expression()
	// pop something 0
	s.vmWriter.WritePop(TEMP, 0)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	// return
	s.vmWriter.WriteReturn()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	s.Term()

	for s.matches(op) {
		token, _ := s.process(op)
		s.Term()

		switch token.Subtype {
		case jack_tokenizer.SYM_SLASH:
			s.vmWriter.WriteCall("Math.divide", 2)
		case jack_tokenizer.SYM_ASTERISK:
			s.vmWriter.WriteCall("Math.multiply", 2)
		default:
			s.vmWriter.WriteArithmetic(subtypeToOp[token.Subtype])
		}

	}
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)
				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.vmWriter.WriteArithmetic(ADD)
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)

				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
		i, _ := strconv.Atoi(token.Lexeme)
		s.vmWriter.WritePush(CONSTANT, i)
	case jack_tokenizer.STRING_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
		s.vmWriter.WritePush(CONSTANT, len(token.Lexeme))
		s.vmWriter.WriteCall("String.new", 1)
		for _, c := range []byte(token.Lexeme) {
			s.vmWriter.WritePush(CONSTANT, int(c))
			s.vmWriter.WriteCall("String.appendChar", 1)
		}
		// push c
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
			// output op
			tokenName := SUB
			if s.Current().Subtype == jack_tokenizer.SYM_TILDE {
				tokenName = NOT
			}
			s.vmWriter.WriteArithmetic(tokenName)
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
			s.vmWriter.WritePush(CONSTANT, 0)
		case jack_tokenizer.KW_TRUE:
			s.vmWriter.WritePush(CONSTANT, 1)
			s.vmWriter.WriteArithmetic(NEG)
		case jack_tokenizer.KW_THIS:
			s.vmWriter.WritePush(POINTER, 0)
		}
		s.keywordHelper(s.Current().Subtype)
	}

}

func (s *parser) SubroutineCall() {
	mainToken, _ := s.identifierHelper() // class's name or subroutine name, depending on if theres a .
	mainName := mainToken.Lexeme
	secondaryName := ""

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		fn, _ := s.identifierHelper()
		secondaryName = "." + fn.Lexeme
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		n := s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		s.vmWriter.WriteCall(fmt.Sprintf("%s%s", mainName, secondaryName), n)
	}
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y

func ParseGrammar(tokens []jack_tokenizer.Token) func(io.WriteCloser) error {
	return func(w io.WriteCloser) error {
		parser := NewParser(tokens, *NewVMWriter(w))

		err := parser.Parse()
		return err
	}
}
package main

import (
	"fmt"
	"os"
	"strings"

	jack_compiler "github.com/renojcpp/n2t-compiler/compiler"
	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func main() {
	args := os.Args[1:]

	for _, arg := range args {
		dirs, err := os.ReadDir(arg)
		if err != nil {
			fmt.Printf("no such directory: %s", arg)
		}

		for _, entry := range dirs {
			if strings.HasSuffix(entry.Name(), ".jack") {
				file, err := os.Open(fmt.Sprintf("%s/%s", arg, entry.Name()))
				if err != nil {
					fmt.Printf("failed to open file: %s", arg)
				}

				tokens, err := jack_tokenizer.Tokenize(file)

				if err != nil {
					fmt.Printf("failed to tokenize: %s", err)
				}

				toOut := jack_compiler.ParseGrammar(tokens)
				fmt.Println("outputting")
				f, _ := os.Create(fmt.Sprintf("%s/%s", arg, entry.Name()+".vm"))
				err = toOut(f)

				if err != nil {
					fmt.Printf("%s", err.Error())
				}
			}
		}
	}
}
./parser/parser.go -exec cat ./parser/parser.go
./parser/parser_test.go -exec cat ./parser/parser_test.go
./tokenizer/token.go -exec cat ./tokenizer/token.go
./tokenizer/tokenizer.go -exec cat ./tokenizer/tokenizer.go
./tokenizer/tokensubtype_string.go -exec cat ./tokenizer/tokensubtype_string.go
./tokenizer/tokentype_string.go -exec cat ./tokenizer/tokentype_string.go
./tokenwriter/writer.go -exec cat ./tokenwriter/writer.go
./tokenwriter/writer_test.go -exec cat ./tokenwriter/writer_test.go
./compiler/vmwriter.go -exec cat ./compiler/vmwriter.go
./compiler/symboltable.go -exec cat ./compiler/symboltable.go
./compiler/compiler.go -exec cat ./compiler/compiler.go
./main.go -exec cat ./main.go
./parser/parser.go
package jack_parser

import (
	"errors"
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	writer io.Writer
	index  int
	err    error
}

func NewParser(tokens []jack_tokenizer.Token, w io.Writer) *parser {
	return &parser{
		tokens,
		w,
		0,
		nil,
	}
}

func (s *parser) process(pairs []tokenpair) {
	if s.matches(pairs) {
		io.WriteString(s.writer, (s.Current().Lexeme))
	} else {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
	}
	s.Advance()
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)
	s.process(tp)
}

func (s *parser) helper_typeVarName() {
	s.process(typePair)
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) wrap(tag string, f func()) {
	io.WriteString(s.writer, "<"+tag+">")
	f()
	io.WriteString(s.writer, "</"+tag+">")
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<symbol>")
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
	io.WriteString(s.writer, "</symbol>")
}

func (s *parser) identifierHelper() {
	io.WriteString(s.writer, "<identifier>")
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
	io.WriteString(s.writer, "</identifier>")
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<keyword>")
	s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
	io.WriteString(s.writer, "</keyword>")
}

// compiles a Class
func (s *parser) Class() {
	io.WriteString(s.writer, "<class>")

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</class>")
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	io.WriteString(s.writer, "<classVarDec>")

	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
		})
	})

	s.helper_typeVarName()
	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</classVarDec>")
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	io.WriteString(s.writer, "<subroutineDec>")
	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		})
	})

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	s.identifierHelper()

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Parameters()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
	io.WriteString(s.writer, "</subroutineDec>")
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() {
	processTypeVarName := func() {
		s.process(typePair)
		s.identifierHelper()
	}
	io.WriteString(s.writer, "<parameters>")
	if s.matches(typePair) {

		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}
	io.WriteString(s.writer, "</parameters>")
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	io.WriteString(s.writer, "<subroutineBody>")
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}

	s.Statements()

	io.WriteString(s.writer, "</subroutineBody>")
}

// Compiles a var declaration
func (s *parser) VarDec() {
	io.WriteString(s.writer, "<varDec>")
	s.keywordHelper(jack_tokenizer.KW_VAR)

	s.helper_typeVarName()

	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</varDec>")
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		io.WriteString(s.writer, "<statements>")
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
		io.WriteString(s.writer, "</statements>")
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	io.WriteString(s.writer, "<letStatement>")
	s.keywordHelper(jack_tokenizer.KW_LET)
	s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</letStatement>")
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	io.WriteString(s.writer, "<ifStatement>")

	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {

		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	io.WriteString(s.writer, "</ifStatement>")
}

// Compiles a While statement
func (s *parser) While() {
	io.WriteString(s.writer, "<whileStatement>")
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</whileStatement>")
}

// Compiles a Do statement
func (s *parser) Do() {
	io.WriteString(s.writer, "<doStatement>")
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.SubroutineCall()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</doStatement>")
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	io.WriteString(s.writer, "<returnStatement>")
	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</returnStatement>")
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	io.WriteString(s.writer, "<expression>")
	s.Term()
	for s.matches(op) {
		s.process(op)
		s.Term()
	}
	io.WriteString(s.writer, "</expression>")
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	io.WriteString(s.writer, "<term>")
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				s.identifierHelper()
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				s.identifierHelper()
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.STRING_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_TRUE:
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
		case jack_tokenizer.KW_THIS:
			s.keywordHelper(s.Current().Subtype)
		}
	}

	io.WriteString(s.writer, "</term>")
}

func (s *parser) SubroutineCall() {
	io.WriteString(s.writer, "<subroutineCall>")
	s.identifierHelper()

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		s.identifierHelper()
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	}
	io.WriteString(s.writer, "</subroutineCall>")
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	io.WriteString(s.writer, "<expressionList>")
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}
	io.WriteString(s.writer, "</expressionList>")

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y
func ParseGrammar(tokens []jack_tokenizer.Token) func(io.Writer) error {
	return func(w io.Writer) error {
		parser := NewParser(tokens, w)

		err := parser.Parse()
		return err
	}
}
./parser/parser_test.go
package jack_parser

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestParseGrammar(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := ParseGrammar(tokens)
					fmt.Println("outputting")
					err = toOut(os.Stdout)

					if err != nil {
						t.Errorf("%s", err.Error())
					}
				}
			}
		})
	}
}
./tokenizer/token.go
package jack_tokenizer

type Token struct {
	Lexeme    string
	Line      int
	Tokentype TokenType
	Subtype   TokenSubtype
}

func NewToken(lexeme string, line int, tokentype TokenType, subtype TokenSubtype) Token {
	return Token{
		lexeme,
		line,
		tokentype,
		subtype,
	}
}

type TokenType int

const (
	ERROR = -1
)

//go:generate string -type=TokenType
const (
	KEYWORD TokenType = iota
	SYMBOL
	INT_CONSTANT
	STRING_CONSTANT
	IDENTIFIER
)

type TokenSubtype int

//go:generate stringer -type=TokenSubtype
const (
	UNKNOWN TokenSubtype = -1
	NONE    TokenSubtype = iota
	// Keywords
	KW_CLASS
	KW_CONSTRUCTOR
	KW_FUNCTION
	KW_METHOD
	KW_FIELD
	KW_STATIC
	KW_VAR
	KW_INT
	KW_CHAR
	KW_BOOLEAN
	KW_VOID
	KW_TRUE
	KW_FALSE
	KW_NULL
	KW_THIS
	KW_LET
	KW_DO
	KW_IF
	KW_ELSE
	KW_WHILE
	KW_RETURN

	// symbols
	SYM_LEFT_BRACE
	SYM_RIGHT_BRACE
	SYM_LEFT_PAREN
	SYM_RIGHT_PAREN
	SYM_LEFT_BRACK
	SYM_RIGHT_BRACK
	SYM_PERIOD
	SYM_COMMA
	SYM_SEMICOLON
	SYM_PLUS
	SYM_MINUS
	SYM_ASTERISK
	SYM_SLASH
	SYM_AMPERSAND
	SYM_PIPE
	SYM_LESS_THAN
	SYM_GREATER_THAN
	SYM_EQUALS
	SYM_TILDE
)

type tokenpair struct {
	tt TokenType
	st TokenSubtype
}

type luxmap map[string]tokenpair

var mp = luxmap{
	"class":       {KEYWORD, KW_CLASS},
	"constructor": {KEYWORD, KW_CONSTRUCTOR},
	"function":    {KEYWORD, KW_FUNCTION},
	"method":      {KEYWORD, KW_METHOD},
	"field":       {KEYWORD, KW_FIELD},
	"static":      {KEYWORD, KW_STATIC},
	"var":         {KEYWORD, KW_VAR},
	"int":         {KEYWORD, KW_INT},
	"char":        {KEYWORD, KW_CHAR},
	"boolean":     {KEYWORD, KW_BOOLEAN},
	"void":        {KEYWORD, KW_VOID},
	"true":        {KEYWORD, KW_TRUE},
	"false":       {KEYWORD, KW_FALSE},
	"null":        {KEYWORD, KW_NULL},
	"this":        {KEYWORD, KW_THIS},
	"let":         {KEYWORD, KW_LET},
	"do":          {KEYWORD, KW_DO},
	"if":          {KEYWORD, KW_IF},
	"else":        {KEYWORD, KW_ELSE},
	"while":       {KEYWORD, KW_WHILE},
	"return":      {KEYWORD, KW_RETURN},

	"{": {SYMBOL, SYM_LEFT_BRACE},
	"}": {SYMBOL, SYM_RIGHT_BRACE},
	"(": {SYMBOL, SYM_LEFT_PAREN},
	")": {SYMBOL, SYM_RIGHT_PAREN},
	"[": {SYMBOL, SYM_LEFT_BRACK},
	"]": {SYMBOL, SYM_RIGHT_BRACK},
	".": {SYMBOL, SYM_PERIOD},
	",": {SYMBOL, SYM_COMMA},
	";": {SYMBOL, SYM_SEMICOLON},
	"+": {SYMBOL, SYM_PLUS},
	"-": {SYMBOL, SYM_MINUS},
	"*": {SYMBOL, SYM_ASTERISK},
	"/": {SYMBOL, SYM_SLASH},
	"&": {SYMBOL, SYM_AMPERSAND},
	"|": {SYMBOL, SYM_PIPE},
	"<": {SYMBOL, SYM_LESS_THAN},
	">": {SYMBOL, SYM_GREATER_THAN},
	"=": {SYMBOL, SYM_EQUALS},
	"~": {SYMBOL, SYM_TILDE},
}
./tokenizer/tokenizer.go
package jack_tokenizer

import (
	"errors"
	"io"
	"strconv"
	"strings"
)

type scanner struct {
	bytes []byte
	index int
}

func (s *scanner) atEnd() bool {
	return s.index >= len(s.bytes)
}

func (s *scanner) advance() {
	s.index++
}

func (s *scanner) peek() (byte, error) {
	if s.atEnd() || s.index+1 >= len(s.bytes) {
		return 0, errors.New("peek beyond")
	}

	return s.bytes[s.index+1], nil
}

func (s *scanner) current() byte {
	return s.bytes[s.index]
}

func NewScanner(b []byte) scanner {
	return scanner{b, 0}
}

func isNumber(b byte) bool {
	_, err := strconv.ParseInt(string(b), 10, 8)
	return err == nil
}

func isLetter(c byte) bool {
	return (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')
}

func Tokenize(r io.Reader) ([]Token, error) {
	chars, err := io.ReadAll(r)
	if err != nil {
		return nil, err
	}
	var parseError error
	parseError = nil
	scan := NewScanner(chars)
	tokens := make([]Token, 0)
	line := 0

	writeInt := func() (string, error) {
		var ss strings.Builder
		for !scan.atEnd() && isNumber(scan.current()) {
			ss.WriteByte(scan.current())
			scan.advance()
		}

		if !scan.atEnd() && isLetter(scan.current()) {
			return "", errors.New("letter found " + string(scan.current()))
		}

		return ss.String(), nil
	}

	for !scan.atEnd() {
		ch := scan.current()
		pair, ok := mp[string(ch)]
		switch {
		// symbols
		case ch == '/':
			peek, err := scan.peek()
			if err != nil {
				parseError = err
			} else {
				if peek == '/' { // single comment
					for !scan.atEnd() && scan.current() != '\n' {
						scan.advance()
					}
				} else if peek == '*' { // multi line comment
					scan.advance()
					peek, _ = scan.peek()
					for !scan.atEnd() && (scan.current() != '*' || peek != '/') {
						scan.advance()
						peek, _ = scan.peek()
					}
					scan.advance()
				} else {
					tokens = append(tokens, NewToken(string(ch), line, SYMBOL, SYM_SLASH))
				}
			}
			scan.advance()
		case ch == '"':
			var ss strings.Builder
			scan.advance()
			for !scan.atEnd() && (scan.current() != '\n' && scan.current() != '"') {
				ss.WriteByte(scan.current())
				scan.advance()
			}

			sres := ss.String()
			if !scan.atEnd() && scan.current() == '"' {
				tokens = append(tokens, NewToken(sres, line, STRING_CONSTANT, NONE))
				scan.advance()
			} else {
				parseError = errors.New("unterminated string")
				tokens = append(tokens, NewToken(sres, line, ERROR, NONE))
			}
		case ok && pair.tt == SYMBOL:
			tokens = append(tokens, NewToken(string(ch), line, pair.tt, pair.st))
			scan.advance()
		case isNumber(ch):
			tt := INT_CONSTANT
			st := NONE
			numberStr, err := writeInt()
			if err != nil {
				tt = ERROR
				parseError = err
			}
			tokens = append(tokens, NewToken(numberStr, line, TokenType(tt), TokenSubtype(st)))

		case isLetter(ch) || ch == '_': // identifier, or keyword
			var ss strings.Builder
			for !scan.atEnd() && isLetter(scan.current()) {
				ss.WriteByte(scan.current())
				scan.advance()
			}
			sres := ss.String()
			pair, ok = mp[sres]
			if ok { // keyword
				tokens = append(tokens, NewToken(sres, line, pair.tt, pair.st))
			} else { // identifier
				tokens = append(tokens, NewToken(sres, line, IDENTIFIER, NONE))
			}
		default: // whitespace or unrecognized
			scan.advance()
		}

		// scan.advance()
		line++
	}

	return tokens, parseError
}
./tokenizer/tokensubtype_string.go
// Code generated by "stringer -type=TokenSubtype"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[UNKNOWN - -1]
	_ = x[NONE-1]
	_ = x[KW_CLASS-2]
	_ = x[KW_CONSTRUCTOR-3]
	_ = x[KW_FUNCTION-4]
	_ = x[KW_METHOD-5]
	_ = x[KW_FIELD-6]
	_ = x[KW_STATIC-7]
	_ = x[KW_VAR-8]
	_ = x[KW_INT-9]
	_ = x[KW_CHAR-10]
	_ = x[KW_BOOLEAN-11]
	_ = x[KW_VOID-12]
	_ = x[KW_TRUE-13]
	_ = x[KW_FALSE-14]
	_ = x[KW_NULL-15]
	_ = x[KW_THIS-16]
	_ = x[KW_LET-17]
	_ = x[KW_DO-18]
	_ = x[KW_IF-19]
	_ = x[KW_ELSE-20]
	_ = x[KW_WHILE-21]
	_ = x[KW_RETURN-22]
	_ = x[SYM_LEFT_BRACE-23]
	_ = x[SYM_RIGHT_BRACE-24]
	_ = x[SYM_LEFT_PAREN-25]
	_ = x[SYM_RIGHT_PAREN-26]
	_ = x[SYM_LEFT_BRACK-27]
	_ = x[SYM_RIGHT_BRACK-28]
	_ = x[SYM_PERIOD-29]
	_ = x[SYM_COMMA-30]
	_ = x[SYM_SEMICOLON-31]
	_ = x[SYM_PLUS-32]
	_ = x[SYM_MINUS-33]
	_ = x[SYM_ASTERISK-34]
	_ = x[SYM_SLASH-35]
	_ = x[SYM_AMPERSAND-36]
	_ = x[SYM_PIPE-37]
	_ = x[SYM_LESS_THAN-38]
	_ = x[SYM_GREATER_THAN-39]
	_ = x[SYM_EQUALS-40]
	_ = x[SYM_TILDE-41]
}

const (
	_TokenSubtype_name_0 = "UNKNOWN"
	_TokenSubtype_name_1 = "NONEKW_CLASSKW_CONSTRUCTORKW_FUNCTIONKW_METHODKW_FIELDKW_STATICKW_VARKW_INTKW_CHARKW_BOOLEANKW_VOIDKW_TRUEKW_FALSEKW_NULLKW_THISKW_LETKW_DOKW_IFKW_ELSEKW_WHILEKW_RETURNSYM_LEFT_BRACESYM_RIGHT_BRACESYM_LEFT_PARENSYM_RIGHT_PARENSYM_LEFT_BRACKSYM_RIGHT_BRACKSYM_PERIODSYM_COMMASYM_SEMICOLONSYM_PLUSSYM_MINUSSYM_ASTERISKSYM_SLASHSYM_AMPERSANDSYM_PIPESYM_LESS_THANSYM_GREATER_THANSYM_EQUALSSYM_TILDE"
)

var (
	_TokenSubtype_index_1 = [...]uint16{0, 4, 12, 26, 37, 46, 54, 63, 69, 75, 82, 92, 99, 106, 114, 121, 128, 134, 139, 144, 151, 159, 168, 182, 197, 211, 226, 240, 255, 265, 274, 287, 295, 304, 316, 325, 338, 346, 359, 375, 385, 394}
)

func (i TokenSubtype) String() string {
	switch {
	case i == -1:
		return _TokenSubtype_name_0
	case 1 <= i && i <= 41:
		i -= 1
		return _TokenSubtype_name_1[_TokenSubtype_index_1[i]:_TokenSubtype_index_1[i+1]]
	default:
		return "TokenSubtype(" + strconv.FormatInt(int64(i), 10) + ")"
	}
}
./tokenizer/tokentype_string.go
// Code generated by "stringer -type=TokenType"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[KEYWORD-0]
	_ = x[SYMBOL-1]
	_ = x[INT_CONSTANT-2]
	_ = x[STRING_CONSTANT-3]
	_ = x[IDENTIFIER-4]
}

const _TokenType_name = "KEYWORDSYMBOLINT_CONSTANTSTRING_CONSTANTIDENTIFIER"

var _TokenType_index = [...]uint8{0, 7, 13, 25, 40, 50}

func (i TokenType) String() string {
	if i < 0 || i >= TokenType(len(_TokenType_index)-1) {
		return "TokenType(" + strconv.FormatInt(int64(i), 10) + ")"
	}
	return _TokenType_name[_TokenType_index[i]:_TokenType_index[i+1]]
}
./tokenwriter/writer.go
package jack_tokenwriter

import (
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func CreateXMLWriter(tokens []jack_tokenizer.Token) func(io.Writer) {
	var ss strings.Builder

	return func(w io.Writer) {
		for _, token := range tokens {
			tagname, ok := keyword2tag[token.Tokentype]
			if !ok {
				tagname = "unknown"
			}
			escaped, ok := escapeLexeme[token.Lexeme]
			if !ok {
				escaped = token.Lexeme
			}
			ss.WriteString(wrap(tagname, escaped, 1))
		}
		var root strings.Builder
		root.WriteString(wrap("tokens", "\n"+ss.String(), 0))

		io.WriteString(w, root.String())
	}
}

func wrap(tagName, content string, tabs int) string {
	return fmt.Sprintf("%s<%s>%s</%s>\n", strings.Repeat("\t", tabs), tagName, content, tagName)
}

var keyword2tag = map[jack_tokenizer.TokenType]string{
	jack_tokenizer.KEYWORD:         "keyword",
	jack_tokenizer.SYMBOL:          "symbol",
	jack_tokenizer.INT_CONSTANT:    "integerConstant",
	jack_tokenizer.STRING_CONSTANT: "stringConstant",
	jack_tokenizer.IDENTIFIER:      "identifier",
}

var escapeLexeme = map[string]string{
	"<":  "&lt;",
	">":  "&gt;",
	"\"": "&quot;",
	"&":  "&amp;",
}
./tokenwriter/writer_test.go
package jack_tokenwriter

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestCreateXMLWriter(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := CreateXMLWriter(tokens)
					fmt.Println("outputting")
					toOut(os.Stdout)
				}
			}
		})
	}
}
./compiler/vmwriter.go
package jack_compiler

import (
	"fmt"
	"io"
)

type VMWriter struct {
	io.WriteCloser
}

var sm = map[SegmentType]string{
	CONSTANT: "constant",
	ARGUMENT: "arg",
	LOCAL:    "local",
	STATIC_S: "static",
	THIS:     "this",
	THAT:     "that",
	POINTER:  "pointer",
	TEMP:     "temp",
}

var am = map[ArithmeticType]string{
	ADD: "add",
	SUB: "sub",
	NEG: "neg",
	EQ:  "eq",
	GT:  "gt",
	LT:  "lt",
	AND: "and",
	OR:  "or",
	NOT: "not",
}

func (s *VMWriter) WritePush(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("push %s %d\n", sm[segment], index))
}

func (s *VMWriter) WritePop(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("pop %s %d\n", sm[segment], index))
}

func (s *VMWriter) WriteArithmetic(arithmetic ArithmeticType) {
	io.WriteString(s, am[arithmetic]+"\n")
}

func (s *VMWriter) WriteLabel(label string) {
	io.WriteString(s, fmt.Sprintf("label %s\n", label))
}

func (s *VMWriter) WriteGoto(label string) {
	io.WriteString(s, fmt.Sprintf("goto %s\n", label))
}

func (s *VMWriter) WriteIf(label string) {
	io.WriteString(s, fmt.Sprintf("if-goto %s\n", label))
}

func (s *VMWriter) WriteCall(name string, nArgs int) {
	io.WriteString(s, fmt.Sprintf("call %s %d\n", name, nArgs))
}

func (s *VMWriter) WriteFunction(name Name, nvars int) {
	io.WriteString(s, fmt.Sprintf("function %s %d\n", name, nvars))
}

func (s *VMWriter) WriteReturn() {
	io.WriteString(s, "return\n")
}

func NewVMWriter(w io.WriteCloser) *VMWriter {
	return &VMWriter{w}
}
./compiler/symboltable.go
package jack_compiler

import jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"

type Name string

type tableentry struct {
	index  int
	typing string
}

type FieldType int

const (
	STATIC_F FieldType = iota
	FIELD
	ARG
	VAR

	NONE
)

var fieldtoSegment = map[FieldType]SegmentType{
	STATIC_F: STATIC_S,
	FIELD:    THIS,
	VAR:      LOCAL,
	ARG:      ARGUMENT,
}

var constructorTTtoFT = map[jack_tokenizer.TokenSubtype]FieldType{
	jack_tokenizer.KW_STATIC: STATIC_F,
	jack_tokenizer.KW_FIELD:  VAR,
}

var subtypeToOp = map[jack_tokenizer.TokenSubtype]ArithmeticType{
	jack_tokenizer.SYM_PLUS:         ADD,
	jack_tokenizer.SYM_MINUS:        SUB,
	jack_tokenizer.SYM_AMPERSAND:    AND,
	jack_tokenizer.SYM_LESS_THAN:    LT,
	jack_tokenizer.SYM_GREATER_THAN: GT,
	jack_tokenizer.SYM_EQUALS:       EQ,
	jack_tokenizer.SYM_PIPE:         OR,
	jack_tokenizer.SYM_TILDE:        NOT,
}

type SymbolTable struct {
	staticTable map[Name]tableentry
	fieldTable  map[Name]tableentry
	argTable    map[Name]tableentry
	varTable    map[Name]tableentry

	staticIndex int
	fieldIndex  int
	argIndex    int
	varIndex    int
}

func (s *SymbolTable) getTable(kind FieldType) (*map[Name]tableentry, *int) {
	switch kind {
	case STATIC_F:
		return &s.staticTable, &s.staticIndex
	case FIELD:
		return &s.fieldTable, &s.fieldIndex
	case ARG:
		return &s.argTable, &s.argIndex
	case VAR:
		return &s.varTable, &s.varIndex
	default:
		panic("unknown segment")
	}
}

func (sym *SymbolTable) find(n Name) *map[Name]tableentry {
	if _, ok := sym.staticTable[n]; ok {
		return &sym.staticTable
	}

	if _, ok := sym.fieldTable[n]; ok {
		return &sym.fieldTable
	}

	if _, ok := sym.argTable[n]; ok {
		return &sym.argTable
	}

	if _, ok := sym.varTable[n]; ok {
		return &sym.varTable
	}

	return nil
}

func (sym *SymbolTable) Reset() {
	sym.staticTable = make(map[Name]tableentry)
	sym.fieldTable = make(map[Name]tableentry)
	sym.argTable = make(map[Name]tableentry)
	sym.varTable = make(map[Name]tableentry)

	sym.staticIndex = 0
	sym.fieldIndex = 0
	sym.argIndex = 0
	sym.varIndex = 0
}

func (sym *SymbolTable) Define(n Name, t string, kind FieldType) {
	table, index := sym.getTable(kind)

	(*table)[n] = tableentry{*index, t}
	*index += 1
}

func (sym *SymbolTable) VarCount(kind FieldType) int {
	table, _ := sym.getTable(kind)

	return len(*table)
}

func (sym *SymbolTable) KindOf(n Name) FieldType {
	if _, ok := sym.staticTable[n]; ok {
		return STATIC_F
	}

	if _, ok := sym.fieldTable[n]; ok {
		return FIELD
	}

	if _, ok := sym.argTable[n]; ok {
		return ARG
	}

	if _, ok := sym.varTable[n]; ok {
		return VAR
	}

	return NONE
}

func (sym *SymbolTable) TypeOf(n Name) string {
	table := sym.find(n)

	return (*table)[n].typing
}

func (sym *SymbolTable) IndexOf(n Name) int {
	table := sym.find(n)

	return (*table)[n].index
}

func NewSymbolTable() *SymbolTable {
	return &SymbolTable{}
}

type SegmentType int

const (
	CONSTANT SegmentType = iota
	ARGUMENT
	LOCAL
	STATIC_S
	THIS
	THAT
	POINTER
	TEMP
)

type ArithmeticType int

const (
	ADD ArithmeticType = iota
	SUB
	NEG
	EQ
	GT
	LT
	AND
	OR
	NOT
)
./compiler/compiler.go
package jack_compiler

import (
	"errors"
	"fmt"
	"io"
	"strconv"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	index  int

	err          error
	subroutineSt *SymbolTable
	classSt      *SymbolTable
	vmWriter     VMWriter

	className   string
	labelNumber int
}

type symboldata struct {
	name   string
	symbol FieldType
	index  int
}

func NewParser(tokens []jack_tokenizer.Token, vmw VMWriter) *parser {
	return &parser{
		tokens,
		0,
		nil,
		NewSymbolTable(),
		NewSymbolTable(),
		vmw,
		"",
		0,
	}
}

func (s *parser) resolveSymbol(sym string) symboldata {
	res := s.subroutineSt.KindOf(Name(sym))

	if res == NONE {
		res = s.classSt.KindOf(Name(sym))
		if res == NONE {
			return symboldata{
				sym,
				NONE,
				0,
			}
		}

		return symboldata{
			sym,
			res,
			s.classSt.IndexOf(Name(sym)),
		}
	}

	return symboldata{
		sym,
		res,
		s.subroutineSt.IndexOf(Name(sym)),
	}
}

func (s *parser) process(pairs []tokenpair) (*jack_tokenizer.Token, error) {
	var err error
	if !s.matches(pairs) {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
		err = s.err
	}
	ret := s.Current()
	s.Advance()

	return ret, err
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) (*jack_tokenizer.Token, error) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)

	return s.process(tp)
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
}

func (s *parser) identifierHelper() (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
}

// compiles a Class
func (s *parser) Class() {
	s.classSt.Reset()
	s.subroutineSt.Reset()

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	token, err := s.identifierHelper()
	if err == nil {
		s.className = token.Lexeme
	}
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	var ft FieldType
	var typing string
	var varName string
	hasError := false
	token, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	})

	if err == nil {
		ft = constructorTTtoFT[token.Subtype]
	} else {
		hasError = true
	}

	token, err = s.helper_type([]tokenpair{})

	if err == nil {
		typing = token.Lexeme
	} else {
		hasError = true
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	} else {
		hasError = true
	}

	if !hasError {
		s.classSt.Define(Name(varName), typing, ft)
	}

	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
			s.classSt.Define(Name(varName), typing, ft)
		}
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	s.subroutineSt.Reset()

	keyToken, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
	})

	if err == nil && keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.subroutineSt.Define("this", s.className, ARG)
	}

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	token, _ := s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	nargs := s.Parameters()

	s.vmWriter.WriteFunction(Name(fmt.Sprintf("%s.%s", s.className, token.Lexeme)), nargs)

	if keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.vmWriter.WritePush(ARGUMENT, 0)
		s.vmWriter.WritePop(POINTER, 0)
	} else if keyToken.Subtype == jack_tokenizer.KW_CONSTRUCTOR {
		n := s.subroutineSt.VarCount(FIELD)
		s.vmWriter.WritePush(CONSTANT, n)
		s.vmWriter.WriteCall("Memory.alloc", 1)
		s.vmWriter.WritePop(POINTER, 0)
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() int {
	count := 0
	processTypeVarName := func() {
		var typing string
		var varName string
		token, err := s.process(typePair)
		if err == nil {
			typing = token.Lexeme
		}
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
		}

		s.subroutineSt.Define(Name(varName), typing, ARG)
		count++
	}
	if s.matches(typePair) {
		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}

	return count
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}
	s.Statements()
}

// Compiles a var declaration
func (s *parser) VarDec() {
	s.keywordHelper(jack_tokenizer.KW_VAR)
	var typing string
	var varName string

	token, err := s.helper_type(nil)
	if err == nil {
		typing = token.Lexeme
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	}

	s.subroutineSt.Define(Name(varName), typing, FieldType(VAR))
	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	s.keywordHelper(jack_tokenizer.KW_LET)
	token, _ := s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	// pop symbolArgName index

	res := s.resolveSymbol(token.Lexeme)
	s.vmWriter.WritePop(fieldtoSegment[res.symbol], res.index)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	// if-goto label1
	s.vmWriter.WriteIf(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.labelNumber++
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()
	// goto label2
	s.vmWriter.WriteGoto(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {
		// label l1
		s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber-1))
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
}

// Compiles a While statement
func (s *parser) While() {
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	s.labelNumber++
	// if-goto l2
	s.vmWriter.WriteIf(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	// goto l1
	s.vmWriter.WriteGoto(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber-1))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
}

// Compiles a Do statement
func (s *parser) Do() {
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.Expression()
	// pop something 0
	s.vmWriter.WritePop(TEMP, 0)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	// return
	s.vmWriter.WriteReturn()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	s.Term()

	for s.matches(op) {
		token, _ := s.process(op)
		s.Term()

		switch token.Subtype {
		case jack_tokenizer.SYM_SLASH:
			s.vmWriter.WriteCall("Math.divide", 2)
		case jack_tokenizer.SYM_ASTERISK:
			s.vmWriter.WriteCall("Math.multiply", 2)
		default:
			s.vmWriter.WriteArithmetic(subtypeToOp[token.Subtype])
		}

	}
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)
				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.vmWriter.WriteArithmetic(ADD)
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)

				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
		i, _ := strconv.Atoi(token.Lexeme)
		s.vmWriter.WritePush(CONSTANT, i)
	case jack_tokenizer.STRING_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
		s.vmWriter.WritePush(CONSTANT, len(token.Lexeme))
		s.vmWriter.WriteCall("String.new", 1)
		for _, c := range []byte(token.Lexeme) {
			s.vmWriter.WritePush(CONSTANT, int(c))
			s.vmWriter.WriteCall("String.appendChar", 1)
		}
		// push c
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
			// output op
			tokenName := SUB
			if s.Current().Subtype == jack_tokenizer.SYM_TILDE {
				tokenName = NOT
			}
			s.vmWriter.WriteArithmetic(tokenName)
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
			s.vmWriter.WritePush(CONSTANT, 0)
		case jack_tokenizer.KW_TRUE:
			s.vmWriter.WritePush(CONSTANT, 1)
			s.vmWriter.WriteArithmetic(NEG)
		case jack_tokenizer.KW_THIS:
			s.vmWriter.WritePush(POINTER, 0)
		}
		s.keywordHelper(s.Current().Subtype)
	}

}

func (s *parser) SubroutineCall() {
	mainToken, _ := s.identifierHelper() // class's name or subroutine name, depending on if theres a .
	mainName := mainToken.Lexeme
	secondaryName := ""

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		fn, _ := s.identifierHelper()
		secondaryName = "." + fn.Lexeme
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		n := s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		s.vmWriter.WriteCall(fmt.Sprintf("%s%s", mainName, secondaryName), n)
	}
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y

func ParseGrammar(tokens []jack_tokenizer.Token) func(io.WriteCloser) error {
	return func(w io.WriteCloser) error {
		parser := NewParser(tokens, *NewVMWriter(w))

		err := parser.Parse()
		return err
	}
}
./main.go
package main

import (
	"fmt"
	"os"
	"strings"

	jack_compiler "github.com/renojcpp/n2t-compiler/compiler"
	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func main() {
	args := os.Args[1:]

	for _, arg := range args {
		dirs, err := os.ReadDir(arg)
		if err != nil {
			fmt.Printf("no such directory: %s", arg)
		}

		for _, entry := range dirs {
			if strings.HasSuffix(entry.Name(), ".jack") {
				file, err := os.Open(fmt.Sprintf("%s/%s", arg, entry.Name()))
				if err != nil {
					fmt.Printf("failed to open file: %s", arg)
				}

				tokens, err := jack_tokenizer.Tokenize(file)

				if err != nil {
					fmt.Printf("failed to tokenize: %s", err)
				}

				toOut := jack_compiler.ParseGrammar(tokens)
				fmt.Println("outputting")
				f, _ := os.Create(fmt.Sprintf("%s/%s", arg, entry.Name()+".vm"))
				err = toOut(f)

				if err != nil {
					fmt.Printf("%s", err.Error())
				}
			}
		}
	}
}
./everything.txt
./parser/parser.go
package jack_parser

import (
	"errors"
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	writer io.Writer
	index  int
	err    error
}

func NewParser(tokens []jack_tokenizer.Token, w io.Writer) *parser {
	return &parser{
		tokens,
		w,
		0,
		nil,
	}
}

func (s *parser) process(pairs []tokenpair) {
	if s.matches(pairs) {
		io.WriteString(s.writer, (s.Current().Lexeme))
	} else {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
	}
	s.Advance()
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)
	s.process(tp)
}

func (s *parser) helper_typeVarName() {
	s.process(typePair)
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) wrap(tag string, f func()) {
	io.WriteString(s.writer, "<"+tag+">")
	f()
	io.WriteString(s.writer, "</"+tag+">")
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<symbol>")
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
	io.WriteString(s.writer, "</symbol>")
}

func (s *parser) identifierHelper() {
	io.WriteString(s.writer, "<identifier>")
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
	io.WriteString(s.writer, "</identifier>")
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<keyword>")
	s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
	io.WriteString(s.writer, "</keyword>")
}

// compiles a Class
func (s *parser) Class() {
	io.WriteString(s.writer, "<class>")

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</class>")
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	io.WriteString(s.writer, "<classVarDec>")

	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
		})
	})

	s.helper_typeVarName()
	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</classVarDec>")
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	io.WriteString(s.writer, "<subroutineDec>")
	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		})
	})

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	s.identifierHelper()

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Parameters()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
	io.WriteString(s.writer, "</subroutineDec>")
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() {
	processTypeVarName := func() {
		s.process(typePair)
		s.identifierHelper()
	}
	io.WriteString(s.writer, "<parameters>")
	if s.matches(typePair) {

		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}
	io.WriteString(s.writer, "</parameters>")
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	io.WriteString(s.writer, "<subroutineBody>")
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}

	s.Statements()

	io.WriteString(s.writer, "</subroutineBody>")
}

// Compiles a var declaration
func (s *parser) VarDec() {
	io.WriteString(s.writer, "<varDec>")
	s.keywordHelper(jack_tokenizer.KW_VAR)

	s.helper_typeVarName()

	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</varDec>")
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		io.WriteString(s.writer, "<statements>")
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
		io.WriteString(s.writer, "</statements>")
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	io.WriteString(s.writer, "<letStatement>")
	s.keywordHelper(jack_tokenizer.KW_LET)
	s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</letStatement>")
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	io.WriteString(s.writer, "<ifStatement>")

	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {

		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	io.WriteString(s.writer, "</ifStatement>")
}

// Compiles a While statement
func (s *parser) While() {
	io.WriteString(s.writer, "<whileStatement>")
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</whileStatement>")
}

// Compiles a Do statement
func (s *parser) Do() {
	io.WriteString(s.writer, "<doStatement>")
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.SubroutineCall()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</doStatement>")
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	io.WriteString(s.writer, "<returnStatement>")
	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</returnStatement>")
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	io.WriteString(s.writer, "<expression>")
	s.Term()
	for s.matches(op) {
		s.process(op)
		s.Term()
	}
	io.WriteString(s.writer, "</expression>")
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	io.WriteString(s.writer, "<term>")
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				s.identifierHelper()
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				s.identifierHelper()
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.STRING_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_TRUE:
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
		case jack_tokenizer.KW_THIS:
			s.keywordHelper(s.Current().Subtype)
		}
	}

	io.WriteString(s.writer, "</term>")
}

func (s *parser) SubroutineCall() {
	io.WriteString(s.writer, "<subroutineCall>")
	s.identifierHelper()

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		s.identifierHelper()
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	}
	io.WriteString(s.writer, "</subroutineCall>")
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	io.WriteString(s.writer, "<expressionList>")
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}
	io.WriteString(s.writer, "</expressionList>")

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y
func ParseGrammar(tokens []jack_tokenizer.Token) func(io.Writer) error {
	return func(w io.Writer) error {
		parser := NewParser(tokens, w)

		err := parser.Parse()
		return err
	}
}
./parser/parser_test.go
package jack_parser

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestParseGrammar(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := ParseGrammar(tokens)
					fmt.Println("outputting")
					err = toOut(os.Stdout)

					if err != nil {
						t.Errorf("%s", err.Error())
					}
				}
			}
		})
	}
}
./tokenizer/token.go
package jack_tokenizer

type Token struct {
	Lexeme    string
	Line      int
	Tokentype TokenType
	Subtype   TokenSubtype
}

func NewToken(lexeme string, line int, tokentype TokenType, subtype TokenSubtype) Token {
	return Token{
		lexeme,
		line,
		tokentype,
		subtype,
	}
}

type TokenType int

const (
	ERROR = -1
)

//go:generate string -type=TokenType
const (
	KEYWORD TokenType = iota
	SYMBOL
	INT_CONSTANT
	STRING_CONSTANT
	IDENTIFIER
)

type TokenSubtype int

//go:generate stringer -type=TokenSubtype
const (
	UNKNOWN TokenSubtype = -1
	NONE    TokenSubtype = iota
	// Keywords
	KW_CLASS
	KW_CONSTRUCTOR
	KW_FUNCTION
	KW_METHOD
	KW_FIELD
	KW_STATIC
	KW_VAR
	KW_INT
	KW_CHAR
	KW_BOOLEAN
	KW_VOID
	KW_TRUE
	KW_FALSE
	KW_NULL
	KW_THIS
	KW_LET
	KW_DO
	KW_IF
	KW_ELSE
	KW_WHILE
	KW_RETURN

	// symbols
	SYM_LEFT_BRACE
	SYM_RIGHT_BRACE
	SYM_LEFT_PAREN
	SYM_RIGHT_PAREN
	SYM_LEFT_BRACK
	SYM_RIGHT_BRACK
	SYM_PERIOD
	SYM_COMMA
	SYM_SEMICOLON
	SYM_PLUS
	SYM_MINUS
	SYM_ASTERISK
	SYM_SLASH
	SYM_AMPERSAND
	SYM_PIPE
	SYM_LESS_THAN
	SYM_GREATER_THAN
	SYM_EQUALS
	SYM_TILDE
)

type tokenpair struct {
	tt TokenType
	st TokenSubtype
}

type luxmap map[string]tokenpair

var mp = luxmap{
	"class":       {KEYWORD, KW_CLASS},
	"constructor": {KEYWORD, KW_CONSTRUCTOR},
	"function":    {KEYWORD, KW_FUNCTION},
	"method":      {KEYWORD, KW_METHOD},
	"field":       {KEYWORD, KW_FIELD},
	"static":      {KEYWORD, KW_STATIC},
	"var":         {KEYWORD, KW_VAR},
	"int":         {KEYWORD, KW_INT},
	"char":        {KEYWORD, KW_CHAR},
	"boolean":     {KEYWORD, KW_BOOLEAN},
	"void":        {KEYWORD, KW_VOID},
	"true":        {KEYWORD, KW_TRUE},
	"false":       {KEYWORD, KW_FALSE},
	"null":        {KEYWORD, KW_NULL},
	"this":        {KEYWORD, KW_THIS},
	"let":         {KEYWORD, KW_LET},
	"do":          {KEYWORD, KW_DO},
	"if":          {KEYWORD, KW_IF},
	"else":        {KEYWORD, KW_ELSE},
	"while":       {KEYWORD, KW_WHILE},
	"return":      {KEYWORD, KW_RETURN},

	"{": {SYMBOL, SYM_LEFT_BRACE},
	"}": {SYMBOL, SYM_RIGHT_BRACE},
	"(": {SYMBOL, SYM_LEFT_PAREN},
	")": {SYMBOL, SYM_RIGHT_PAREN},
	"[": {SYMBOL, SYM_LEFT_BRACK},
	"]": {SYMBOL, SYM_RIGHT_BRACK},
	".": {SYMBOL, SYM_PERIOD},
	",": {SYMBOL, SYM_COMMA},
	";": {SYMBOL, SYM_SEMICOLON},
	"+": {SYMBOL, SYM_PLUS},
	"-": {SYMBOL, SYM_MINUS},
	"*": {SYMBOL, SYM_ASTERISK},
	"/": {SYMBOL, SYM_SLASH},
	"&": {SYMBOL, SYM_AMPERSAND},
	"|": {SYMBOL, SYM_PIPE},
	"<": {SYMBOL, SYM_LESS_THAN},
	">": {SYMBOL, SYM_GREATER_THAN},
	"=": {SYMBOL, SYM_EQUALS},
	"~": {SYMBOL, SYM_TILDE},
}
./tokenizer/tokenizer.go
package jack_tokenizer

import (
	"errors"
	"io"
	"strconv"
	"strings"
)

type scanner struct {
	bytes []byte
	index int
}

func (s *scanner) atEnd() bool {
	return s.index >= len(s.bytes)
}

func (s *scanner) advance() {
	s.index++
}

func (s *scanner) peek() (byte, error) {
	if s.atEnd() || s.index+1 >= len(s.bytes) {
		return 0, errors.New("peek beyond")
	}

	return s.bytes[s.index+1], nil
}

func (s *scanner) current() byte {
	return s.bytes[s.index]
}

func NewScanner(b []byte) scanner {
	return scanner{b, 0}
}

func isNumber(b byte) bool {
	_, err := strconv.ParseInt(string(b), 10, 8)
	return err == nil
}

func isLetter(c byte) bool {
	return (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')
}

func Tokenize(r io.Reader) ([]Token, error) {
	chars, err := io.ReadAll(r)
	if err != nil {
		return nil, err
	}
	var parseError error
	parseError = nil
	scan := NewScanner(chars)
	tokens := make([]Token, 0)
	line := 0

	writeInt := func() (string, error) {
		var ss strings.Builder
		for !scan.atEnd() && isNumber(scan.current()) {
			ss.WriteByte(scan.current())
			scan.advance()
		}

		if !scan.atEnd() && isLetter(scan.current()) {
			return "", errors.New("letter found " + string(scan.current()))
		}

		return ss.String(), nil
	}

	for !scan.atEnd() {
		ch := scan.current()
		pair, ok := mp[string(ch)]
		switch {
		// symbols
		case ch == '/':
			peek, err := scan.peek()
			if err != nil {
				parseError = err
			} else {
				if peek == '/' { // single comment
					for !scan.atEnd() && scan.current() != '\n' {
						scan.advance()
					}
				} else if peek == '*' { // multi line comment
					scan.advance()
					peek, _ = scan.peek()
					for !scan.atEnd() && (scan.current() != '*' || peek != '/') {
						scan.advance()
						peek, _ = scan.peek()
					}
					scan.advance()
				} else {
					tokens = append(tokens, NewToken(string(ch), line, SYMBOL, SYM_SLASH))
				}
			}
			scan.advance()
		case ch == '"':
			var ss strings.Builder
			scan.advance()
			for !scan.atEnd() && (scan.current() != '\n' && scan.current() != '"') {
				ss.WriteByte(scan.current())
				scan.advance()
			}

			sres := ss.String()
			if !scan.atEnd() && scan.current() == '"' {
				tokens = append(tokens, NewToken(sres, line, STRING_CONSTANT, NONE))
				scan.advance()
			} else {
				parseError = errors.New("unterminated string")
				tokens = append(tokens, NewToken(sres, line, ERROR, NONE))
			}
		case ok && pair.tt == SYMBOL:
			tokens = append(tokens, NewToken(string(ch), line, pair.tt, pair.st))
			scan.advance()
		case isNumber(ch):
			tt := INT_CONSTANT
			st := NONE
			numberStr, err := writeInt()
			if err != nil {
				tt = ERROR
				parseError = err
			}
			tokens = append(tokens, NewToken(numberStr, line, TokenType(tt), TokenSubtype(st)))

		case isLetter(ch) || ch == '_': // identifier, or keyword
			var ss strings.Builder
			for !scan.atEnd() && isLetter(scan.current()) {
				ss.WriteByte(scan.current())
				scan.advance()
			}
			sres := ss.String()
			pair, ok = mp[sres]
			if ok { // keyword
				tokens = append(tokens, NewToken(sres, line, pair.tt, pair.st))
			} else { // identifier
				tokens = append(tokens, NewToken(sres, line, IDENTIFIER, NONE))
			}
		default: // whitespace or unrecognized
			scan.advance()
		}

		// scan.advance()
		line++
	}

	return tokens, parseError
}
./tokenizer/tokensubtype_string.go
// Code generated by "stringer -type=TokenSubtype"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[UNKNOWN - -1]
	_ = x[NONE-1]
	_ = x[KW_CLASS-2]
	_ = x[KW_CONSTRUCTOR-3]
	_ = x[KW_FUNCTION-4]
	_ = x[KW_METHOD-5]
	_ = x[KW_FIELD-6]
	_ = x[KW_STATIC-7]
	_ = x[KW_VAR-8]
	_ = x[KW_INT-9]
	_ = x[KW_CHAR-10]
	_ = x[KW_BOOLEAN-11]
	_ = x[KW_VOID-12]
	_ = x[KW_TRUE-13]
	_ = x[KW_FALSE-14]
	_ = x[KW_NULL-15]
	_ = x[KW_THIS-16]
	_ = x[KW_LET-17]
	_ = x[KW_DO-18]
	_ = x[KW_IF-19]
	_ = x[KW_ELSE-20]
	_ = x[KW_WHILE-21]
	_ = x[KW_RETURN-22]
	_ = x[SYM_LEFT_BRACE-23]
	_ = x[SYM_RIGHT_BRACE-24]
	_ = x[SYM_LEFT_PAREN-25]
	_ = x[SYM_RIGHT_PAREN-26]
	_ = x[SYM_LEFT_BRACK-27]
	_ = x[SYM_RIGHT_BRACK-28]
	_ = x[SYM_PERIOD-29]
	_ = x[SYM_COMMA-30]
	_ = x[SYM_SEMICOLON-31]
	_ = x[SYM_PLUS-32]
	_ = x[SYM_MINUS-33]
	_ = x[SYM_ASTERISK-34]
	_ = x[SYM_SLASH-35]
	_ = x[SYM_AMPERSAND-36]
	_ = x[SYM_PIPE-37]
	_ = x[SYM_LESS_THAN-38]
	_ = x[SYM_GREATER_THAN-39]
	_ = x[SYM_EQUALS-40]
	_ = x[SYM_TILDE-41]
}

const (
	_TokenSubtype_name_0 = "UNKNOWN"
	_TokenSubtype_name_1 = "NONEKW_CLASSKW_CONSTRUCTORKW_FUNCTIONKW_METHODKW_FIELDKW_STATICKW_VARKW_INTKW_CHARKW_BOOLEANKW_VOIDKW_TRUEKW_FALSEKW_NULLKW_THISKW_LETKW_DOKW_IFKW_ELSEKW_WHILEKW_RETURNSYM_LEFT_BRACESYM_RIGHT_BRACESYM_LEFT_PARENSYM_RIGHT_PARENSYM_LEFT_BRACKSYM_RIGHT_BRACKSYM_PERIODSYM_COMMASYM_SEMICOLONSYM_PLUSSYM_MINUSSYM_ASTERISKSYM_SLASHSYM_AMPERSANDSYM_PIPESYM_LESS_THANSYM_GREATER_THANSYM_EQUALSSYM_TILDE"
)

var (
	_TokenSubtype_index_1 = [...]uint16{0, 4, 12, 26, 37, 46, 54, 63, 69, 75, 82, 92, 99, 106, 114, 121, 128, 134, 139, 144, 151, 159, 168, 182, 197, 211, 226, 240, 255, 265, 274, 287, 295, 304, 316, 325, 338, 346, 359, 375, 385, 394}
)

func (i TokenSubtype) String() string {
	switch {
	case i == -1:
		return _TokenSubtype_name_0
	case 1 <= i && i <= 41:
		i -= 1
		return _TokenSubtype_name_1[_TokenSubtype_index_1[i]:_TokenSubtype_index_1[i+1]]
	default:
		return "TokenSubtype(" + strconv.FormatInt(int64(i), 10) + ")"
	}
}
./tokenizer/tokentype_string.go
// Code generated by "stringer -type=TokenType"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[KEYWORD-0]
	_ = x[SYMBOL-1]
	_ = x[INT_CONSTANT-2]
	_ = x[STRING_CONSTANT-3]
	_ = x[IDENTIFIER-4]
}

const _TokenType_name = "KEYWORDSYMBOLINT_CONSTANTSTRING_CONSTANTIDENTIFIER"

var _TokenType_index = [...]uint8{0, 7, 13, 25, 40, 50}

func (i TokenType) String() string {
	if i < 0 || i >= TokenType(len(_TokenType_index)-1) {
		return "TokenType(" + strconv.FormatInt(int64(i), 10) + ")"
	}
	return _TokenType_name[_TokenType_index[i]:_TokenType_index[i+1]]
}
./tokenwriter/writer.go
package jack_tokenwriter

import (
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func CreateXMLWriter(tokens []jack_tokenizer.Token) func(io.Writer) {
	var ss strings.Builder

	return func(w io.Writer) {
		for _, token := range tokens {
			tagname, ok := keyword2tag[token.Tokentype]
			if !ok {
				tagname = "unknown"
			}
			escaped, ok := escapeLexeme[token.Lexeme]
			if !ok {
				escaped = token.Lexeme
			}
			ss.WriteString(wrap(tagname, escaped, 1))
		}
		var root strings.Builder
		root.WriteString(wrap("tokens", "\n"+ss.String(), 0))

		io.WriteString(w, root.String())
	}
}

func wrap(tagName, content string, tabs int) string {
	return fmt.Sprintf("%s<%s>%s</%s>\n", strings.Repeat("\t", tabs), tagName, content, tagName)
}

var keyword2tag = map[jack_tokenizer.TokenType]string{
	jack_tokenizer.KEYWORD:         "keyword",
	jack_tokenizer.SYMBOL:          "symbol",
	jack_tokenizer.INT_CONSTANT:    "integerConstant",
	jack_tokenizer.STRING_CONSTANT: "stringConstant",
	jack_tokenizer.IDENTIFIER:      "identifier",
}

var escapeLexeme = map[string]string{
	"<":  "&lt;",
	">":  "&gt;",
	"\"": "&quot;",
	"&":  "&amp;",
}
./tokenwriter/writer_test.go
package jack_tokenwriter

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestCreateXMLWriter(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := CreateXMLWriter(tokens)
					fmt.Println("outputting")
					toOut(os.Stdout)
				}
			}
		})
	}
}
./compiler/vmwriter.go
package jack_compiler

import (
	"fmt"
	"io"
)

type VMWriter struct {
	io.WriteCloser
}

var sm = map[SegmentType]string{
	CONSTANT: "constant",
	ARGUMENT: "arg",
	LOCAL:    "local",
	STATIC_S: "static",
	THIS:     "this",
	THAT:     "that",
	POINTER:  "pointer",
	TEMP:     "temp",
}

var am = map[ArithmeticType]string{
	ADD: "add",
	SUB: "sub",
	NEG: "neg",
	EQ:  "eq",
	GT:  "gt",
	LT:  "lt",
	AND: "and",
	OR:  "or",
	NOT: "not",
}

func (s *VMWriter) WritePush(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("push %s %d\n", sm[segment], index))
}

func (s *VMWriter) WritePop(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("pop %s %d\n", sm[segment], index))
}

func (s *VMWriter) WriteArithmetic(arithmetic ArithmeticType) {
	io.WriteString(s, am[arithmetic]+"\n")
}

func (s *VMWriter) WriteLabel(label string) {
	io.WriteString(s, fmt.Sprintf("label %s\n", label))
}

func (s *VMWriter) WriteGoto(label string) {
	io.WriteString(s, fmt.Sprintf("goto %s\n", label))
}

func (s *VMWriter) WriteIf(label string) {
	io.WriteString(s, fmt.Sprintf("if-goto %s\n", label))
}

func (s *VMWriter) WriteCall(name string, nArgs int) {
	io.WriteString(s, fmt.Sprintf("call %s %d\n", name, nArgs))
}

func (s *VMWriter) WriteFunction(name Name, nvars int) {
	io.WriteString(s, fmt.Sprintf("function %s %d\n", name, nvars))
}

func (s *VMWriter) WriteReturn() {
	io.WriteString(s, "return\n")
}

func NewVMWriter(w io.WriteCloser) *VMWriter {
	return &VMWriter{w}
}
./compiler/symboltable.go
package jack_compiler

import jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"

type Name string

type tableentry struct {
	index  int
	typing string
}

type FieldType int

const (
	STATIC_F FieldType = iota
	FIELD
	ARG
	VAR

	NONE
)

var fieldtoSegment = map[FieldType]SegmentType{
	STATIC_F: STATIC_S,
	FIELD:    THIS,
	VAR:      LOCAL,
	ARG:      ARGUMENT,
}

var constructorTTtoFT = map[jack_tokenizer.TokenSubtype]FieldType{
	jack_tokenizer.KW_STATIC: STATIC_F,
	jack_tokenizer.KW_FIELD:  VAR,
}

var subtypeToOp = map[jack_tokenizer.TokenSubtype]ArithmeticType{
	jack_tokenizer.SYM_PLUS:         ADD,
	jack_tokenizer.SYM_MINUS:        SUB,
	jack_tokenizer.SYM_AMPERSAND:    AND,
	jack_tokenizer.SYM_LESS_THAN:    LT,
	jack_tokenizer.SYM_GREATER_THAN: GT,
	jack_tokenizer.SYM_EQUALS:       EQ,
	jack_tokenizer.SYM_PIPE:         OR,
	jack_tokenizer.SYM_TILDE:        NOT,
}

type SymbolTable struct {
	staticTable map[Name]tableentry
	fieldTable  map[Name]tableentry
	argTable    map[Name]tableentry
	varTable    map[Name]tableentry

	staticIndex int
	fieldIndex  int
	argIndex    int
	varIndex    int
}

func (s *SymbolTable) getTable(kind FieldType) (*map[Name]tableentry, *int) {
	switch kind {
	case STATIC_F:
		return &s.staticTable, &s.staticIndex
	case FIELD:
		return &s.fieldTable, &s.fieldIndex
	case ARG:
		return &s.argTable, &s.argIndex
	case VAR:
		return &s.varTable, &s.varIndex
	default:
		panic("unknown segment")
	}
}

func (sym *SymbolTable) find(n Name) *map[Name]tableentry {
	if _, ok := sym.staticTable[n]; ok {
		return &sym.staticTable
	}

	if _, ok := sym.fieldTable[n]; ok {
		return &sym.fieldTable
	}

	if _, ok := sym.argTable[n]; ok {
		return &sym.argTable
	}

	if _, ok := sym.varTable[n]; ok {
		return &sym.varTable
	}

	return nil
}

func (sym *SymbolTable) Reset() {
	sym.staticTable = make(map[Name]tableentry)
	sym.fieldTable = make(map[Name]tableentry)
	sym.argTable = make(map[Name]tableentry)
	sym.varTable = make(map[Name]tableentry)

	sym.staticIndex = 0
	sym.fieldIndex = 0
	sym.argIndex = 0
	sym.varIndex = 0
}

func (sym *SymbolTable) Define(n Name, t string, kind FieldType) {
	table, index := sym.getTable(kind)

	(*table)[n] = tableentry{*index, t}
	*index += 1
}

func (sym *SymbolTable) VarCount(kind FieldType) int {
	table, _ := sym.getTable(kind)

	return len(*table)
}

func (sym *SymbolTable) KindOf(n Name) FieldType {
	if _, ok := sym.staticTable[n]; ok {
		return STATIC_F
	}

	if _, ok := sym.fieldTable[n]; ok {
		return FIELD
	}

	if _, ok := sym.argTable[n]; ok {
		return ARG
	}

	if _, ok := sym.varTable[n]; ok {
		return VAR
	}

	return NONE
}

func (sym *SymbolTable) TypeOf(n Name) string {
	table := sym.find(n)

	return (*table)[n].typing
}

func (sym *SymbolTable) IndexOf(n Name) int {
	table := sym.find(n)

	return (*table)[n].index
}

func NewSymbolTable() *SymbolTable {
	return &SymbolTable{}
}

type SegmentType int

const (
	CONSTANT SegmentType = iota
	ARGUMENT
	LOCAL
	STATIC_S
	THIS
	THAT
	POINTER
	TEMP
)

type ArithmeticType int

const (
	ADD ArithmeticType = iota
	SUB
	NEG
	EQ
	GT
	LT
	AND
	OR
	NOT
)
./compiler/compiler.go
package jack_compiler

import (
	"errors"
	"fmt"
	"io"
	"strconv"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	index  int

	err          error
	subroutineSt *SymbolTable
	classSt      *SymbolTable
	vmWriter     VMWriter

	className   string
	labelNumber int
}

type symboldata struct {
	name   string
	symbol FieldType
	index  int
}

func NewParser(tokens []jack_tokenizer.Token, vmw VMWriter) *parser {
	return &parser{
		tokens,
		0,
		nil,
		NewSymbolTable(),
		NewSymbolTable(),
		vmw,
		"",
		0,
	}
}

func (s *parser) resolveSymbol(sym string) symboldata {
	res := s.subroutineSt.KindOf(Name(sym))

	if res == NONE {
		res = s.classSt.KindOf(Name(sym))
		if res == NONE {
			return symboldata{
				sym,
				NONE,
				0,
			}
		}

		return symboldata{
			sym,
			res,
			s.classSt.IndexOf(Name(sym)),
		}
	}

	return symboldata{
		sym,
		res,
		s.subroutineSt.IndexOf(Name(sym)),
	}
}

func (s *parser) process(pairs []tokenpair) (*jack_tokenizer.Token, error) {
	var err error
	if !s.matches(pairs) {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
		err = s.err
	}
	ret := s.Current()
	s.Advance()

	return ret, err
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) (*jack_tokenizer.Token, error) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)

	return s.process(tp)
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
}

func (s *parser) identifierHelper() (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
}

// compiles a Class
func (s *parser) Class() {
	s.classSt.Reset()
	s.subroutineSt.Reset()

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	token, err := s.identifierHelper()
	if err == nil {
		s.className = token.Lexeme
	}
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	var ft FieldType
	var typing string
	var varName string
	hasError := false
	token, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	})

	if err == nil {
		ft = constructorTTtoFT[token.Subtype]
	} else {
		hasError = true
	}

	token, err = s.helper_type([]tokenpair{})

	if err == nil {
		typing = token.Lexeme
	} else {
		hasError = true
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	} else {
		hasError = true
	}

	if !hasError {
		s.classSt.Define(Name(varName), typing, ft)
	}

	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
			s.classSt.Define(Name(varName), typing, ft)
		}
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	s.subroutineSt.Reset()

	keyToken, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
	})

	if err == nil && keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.subroutineSt.Define("this", s.className, ARG)
	}

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	token, _ := s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	nargs := s.Parameters()

	s.vmWriter.WriteFunction(Name(fmt.Sprintf("%s.%s", s.className, token.Lexeme)), nargs)

	if keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.vmWriter.WritePush(ARGUMENT, 0)
		s.vmWriter.WritePop(POINTER, 0)
	} else if keyToken.Subtype == jack_tokenizer.KW_CONSTRUCTOR {
		n := s.subroutineSt.VarCount(FIELD)
		s.vmWriter.WritePush(CONSTANT, n)
		s.vmWriter.WriteCall("Memory.alloc", 1)
		s.vmWriter.WritePop(POINTER, 0)
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() int {
	count := 0
	processTypeVarName := func() {
		var typing string
		var varName string
		token, err := s.process(typePair)
		if err == nil {
			typing = token.Lexeme
		}
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
		}

		s.subroutineSt.Define(Name(varName), typing, ARG)
		count++
	}
	if s.matches(typePair) {
		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}

	return count
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}
	s.Statements()
}

// Compiles a var declaration
func (s *parser) VarDec() {
	s.keywordHelper(jack_tokenizer.KW_VAR)
	var typing string
	var varName string

	token, err := s.helper_type(nil)
	if err == nil {
		typing = token.Lexeme
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	}

	s.subroutineSt.Define(Name(varName), typing, FieldType(VAR))
	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	s.keywordHelper(jack_tokenizer.KW_LET)
	token, _ := s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	// pop symbolArgName index

	res := s.resolveSymbol(token.Lexeme)
	s.vmWriter.WritePop(fieldtoSegment[res.symbol], res.index)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	// if-goto label1
	s.vmWriter.WriteIf(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.labelNumber++
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()
	// goto label2
	s.vmWriter.WriteGoto(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {
		// label l1
		s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber-1))
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
}

// Compiles a While statement
func (s *parser) While() {
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	s.labelNumber++
	// if-goto l2
	s.vmWriter.WriteIf(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	// goto l1
	s.vmWriter.WriteGoto(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber-1))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
}

// Compiles a Do statement
func (s *parser) Do() {
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.Expression()
	// pop something 0
	s.vmWriter.WritePop(TEMP, 0)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	// return
	s.vmWriter.WriteReturn()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	s.Term()

	for s.matches(op) {
		token, _ := s.process(op)
		s.Term()

		switch token.Subtype {
		case jack_tokenizer.SYM_SLASH:
			s.vmWriter.WriteCall("Math.divide", 2)
		case jack_tokenizer.SYM_ASTERISK:
			s.vmWriter.WriteCall("Math.multiply", 2)
		default:
			s.vmWriter.WriteArithmetic(subtypeToOp[token.Subtype])
		}

	}
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)
				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.vmWriter.WriteArithmetic(ADD)
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)

				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
		i, _ := strconv.Atoi(token.Lexeme)
		s.vmWriter.WritePush(CONSTANT, i)
	case jack_tokenizer.STRING_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
		s.vmWriter.WritePush(CONSTANT, len(token.Lexeme))
		s.vmWriter.WriteCall("String.new", 1)
		for _, c := range []byte(token.Lexeme) {
			s.vmWriter.WritePush(CONSTANT, int(c))
			s.vmWriter.WriteCall("String.appendChar", 1)
		}
		// push c
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
			// output op
			tokenName := SUB
			if s.Current().Subtype == jack_tokenizer.SYM_TILDE {
				tokenName = NOT
			}
			s.vmWriter.WriteArithmetic(tokenName)
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
			s.vmWriter.WritePush(CONSTANT, 0)
		case jack_tokenizer.KW_TRUE:
			s.vmWriter.WritePush(CONSTANT, 1)
			s.vmWriter.WriteArithmetic(NEG)
		case jack_tokenizer.KW_THIS:
			s.vmWriter.WritePush(POINTER, 0)
		}
		s.keywordHelper(s.Current().Subtype)
	}

}

func (s *parser) SubroutineCall() {
	mainToken, _ := s.identifierHelper() // class's name or subroutine name, depending on if theres a .
	mainName := mainToken.Lexeme
	secondaryName := ""

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		fn, _ := s.identifierHelper()
		secondaryName = "." + fn.Lexeme
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		n := s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		s.vmWriter.WriteCall(fmt.Sprintf("%s%s", mainName, secondaryName), n)
	}
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y

func ParseGrammar(tokens []jack_tokenizer.Token) func(io.WriteCloser) error {
	return func(w io.WriteCloser) error {
		parser := NewParser(tokens, *NewVMWriter(w))

		err := parser.Parse()
		return err
	}
}
./main.go
package main

import (
	"fmt"
	"os"
	"strings"

	jack_compiler "github.com/renojcpp/n2t-compiler/compiler"
	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func main() {
	args := os.Args[1:]

	for _, arg := range args {
		dirs, err := os.ReadDir(arg)
		if err != nil {
			fmt.Printf("no such directory: %s", arg)
		}

		for _, entry := range dirs {
			if strings.HasSuffix(entry.Name(), ".jack") {
				file, err := os.Open(fmt.Sprintf("%s/%s", arg, entry.Name()))
				if err != nil {
					fmt.Printf("failed to open file: %s", arg)
				}

				tokens, err := jack_tokenizer.Tokenize(file)

				if err != nil {
					fmt.Printf("failed to tokenize: %s", err)
				}

				toOut := jack_compiler.ParseGrammar(tokens)
				fmt.Println("outputting")
				f, _ := os.Create(fmt.Sprintf("%s/%s", arg, entry.Name()+".vm"))
				err = toOut(f)

				if err != nil {
					fmt.Printf("%s", err.Error())
				}
			}
		}
	}
}
./parser/parser.go
package jack_parser

import (
	"errors"
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	writer io.Writer
	index  int
	err    error
}

func NewParser(tokens []jack_tokenizer.Token, w io.Writer) *parser {
	return &parser{
		tokens,
		w,
		0,
		nil,
	}
}

func (s *parser) process(pairs []tokenpair) {
	if s.matches(pairs) {
		io.WriteString(s.writer, (s.Current().Lexeme))
	} else {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
	}
	s.Advance()
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)
	s.process(tp)
}

func (s *parser) helper_typeVarName() {
	s.process(typePair)
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) wrap(tag string, f func()) {
	io.WriteString(s.writer, "<"+tag+">")
	f()
	io.WriteString(s.writer, "</"+tag+">")
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<symbol>")
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
	io.WriteString(s.writer, "</symbol>")
}

func (s *parser) identifierHelper() {
	io.WriteString(s.writer, "<identifier>")
	s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
	io.WriteString(s.writer, "</identifier>")
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) {
	io.WriteString(s.writer, "<keyword>")
	s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
	io.WriteString(s.writer, "</keyword>")
}

// compiles a Class
func (s *parser) Class() {
	io.WriteString(s.writer, "<class>")

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</class>")
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	io.WriteString(s.writer, "<classVarDec>")

	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
		})
	})

	s.helper_typeVarName()
	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</classVarDec>")
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	io.WriteString(s.writer, "<subroutineDec>")
	s.wrap("keyword", func() {
		s.process([]tokenpair{
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
			{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		})
	})

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	s.identifierHelper()

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Parameters()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
	io.WriteString(s.writer, "</subroutineDec>")
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() {
	processTypeVarName := func() {
		s.process(typePair)
		s.identifierHelper()
	}
	io.WriteString(s.writer, "<parameters>")
	if s.matches(typePair) {

		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}
	io.WriteString(s.writer, "</parameters>")
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	io.WriteString(s.writer, "<subroutineBody>")
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}

	s.Statements()

	io.WriteString(s.writer, "</subroutineBody>")
}

// Compiles a var declaration
func (s *parser) VarDec() {
	io.WriteString(s.writer, "<varDec>")
	s.keywordHelper(jack_tokenizer.KW_VAR)

	s.helper_typeVarName()

	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</varDec>")
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		io.WriteString(s.writer, "<statements>")
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
		io.WriteString(s.writer, "</statements>")
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	io.WriteString(s.writer, "<letStatement>")
	s.keywordHelper(jack_tokenizer.KW_LET)
	s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)

	io.WriteString(s.writer, "</letStatement>")
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	io.WriteString(s.writer, "<ifStatement>")

	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {

		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	io.WriteString(s.writer, "</ifStatement>")
}

// Compiles a While statement
func (s *parser) While() {
	io.WriteString(s.writer, "<whileStatement>")
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.Expression()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	io.WriteString(s.writer, "</whileStatement>")
}

// Compiles a Do statement
func (s *parser) Do() {
	io.WriteString(s.writer, "<doStatement>")
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.SubroutineCall()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</doStatement>")
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	io.WriteString(s.writer, "<returnStatement>")
	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
	io.WriteString(s.writer, "</returnStatement>")
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	io.WriteString(s.writer, "<expression>")
	s.Term()
	for s.matches(op) {
		s.process(op)
		s.Term()
	}
	io.WriteString(s.writer, "</expression>")
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	io.WriteString(s.writer, "<term>")
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				s.identifierHelper()
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				s.identifierHelper()
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.STRING_CONSTANT:
		s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_TRUE:
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
		case jack_tokenizer.KW_THIS:
			s.keywordHelper(s.Current().Subtype)
		}
	}

	io.WriteString(s.writer, "</term>")
}

func (s *parser) SubroutineCall() {
	io.WriteString(s.writer, "<subroutineCall>")
	s.identifierHelper()

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		s.identifierHelper()
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	}
	io.WriteString(s.writer, "</subroutineCall>")
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	io.WriteString(s.writer, "<expressionList>")
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}
	io.WriteString(s.writer, "</expressionList>")

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y
func ParseGrammar(tokens []jack_tokenizer.Token) func(io.Writer) error {
	return func(w io.Writer) error {
		parser := NewParser(tokens, w)

		err := parser.Parse()
		return err
	}
}
./parser/parser_test.go
package jack_parser

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestParseGrammar(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := ParseGrammar(tokens)
					fmt.Println("outputting")
					err = toOut(os.Stdout)

					if err != nil {
						t.Errorf("%s", err.Error())
					}
				}
			}
		})
	}
}
./tokenizer/token.go
package jack_tokenizer

type Token struct {
	Lexeme    string
	Line      int
	Tokentype TokenType
	Subtype   TokenSubtype
}

func NewToken(lexeme string, line int, tokentype TokenType, subtype TokenSubtype) Token {
	return Token{
		lexeme,
		line,
		tokentype,
		subtype,
	}
}

type TokenType int

const (
	ERROR = -1
)

//go:generate string -type=TokenType
const (
	KEYWORD TokenType = iota
	SYMBOL
	INT_CONSTANT
	STRING_CONSTANT
	IDENTIFIER
)

type TokenSubtype int

//go:generate stringer -type=TokenSubtype
const (
	UNKNOWN TokenSubtype = -1
	NONE    TokenSubtype = iota
	// Keywords
	KW_CLASS
	KW_CONSTRUCTOR
	KW_FUNCTION
	KW_METHOD
	KW_FIELD
	KW_STATIC
	KW_VAR
	KW_INT
	KW_CHAR
	KW_BOOLEAN
	KW_VOID
	KW_TRUE
	KW_FALSE
	KW_NULL
	KW_THIS
	KW_LET
	KW_DO
	KW_IF
	KW_ELSE
	KW_WHILE
	KW_RETURN

	// symbols
	SYM_LEFT_BRACE
	SYM_RIGHT_BRACE
	SYM_LEFT_PAREN
	SYM_RIGHT_PAREN
	SYM_LEFT_BRACK
	SYM_RIGHT_BRACK
	SYM_PERIOD
	SYM_COMMA
	SYM_SEMICOLON
	SYM_PLUS
	SYM_MINUS
	SYM_ASTERISK
	SYM_SLASH
	SYM_AMPERSAND
	SYM_PIPE
	SYM_LESS_THAN
	SYM_GREATER_THAN
	SYM_EQUALS
	SYM_TILDE
)

type tokenpair struct {
	tt TokenType
	st TokenSubtype
}

type luxmap map[string]tokenpair

var mp = luxmap{
	"class":       {KEYWORD, KW_CLASS},
	"constructor": {KEYWORD, KW_CONSTRUCTOR},
	"function":    {KEYWORD, KW_FUNCTION},
	"method":      {KEYWORD, KW_METHOD},
	"field":       {KEYWORD, KW_FIELD},
	"static":      {KEYWORD, KW_STATIC},
	"var":         {KEYWORD, KW_VAR},
	"int":         {KEYWORD, KW_INT},
	"char":        {KEYWORD, KW_CHAR},
	"boolean":     {KEYWORD, KW_BOOLEAN},
	"void":        {KEYWORD, KW_VOID},
	"true":        {KEYWORD, KW_TRUE},
	"false":       {KEYWORD, KW_FALSE},
	"null":        {KEYWORD, KW_NULL},
	"this":        {KEYWORD, KW_THIS},
	"let":         {KEYWORD, KW_LET},
	"do":          {KEYWORD, KW_DO},
	"if":          {KEYWORD, KW_IF},
	"else":        {KEYWORD, KW_ELSE},
	"while":       {KEYWORD, KW_WHILE},
	"return":      {KEYWORD, KW_RETURN},

	"{": {SYMBOL, SYM_LEFT_BRACE},
	"}": {SYMBOL, SYM_RIGHT_BRACE},
	"(": {SYMBOL, SYM_LEFT_PAREN},
	")": {SYMBOL, SYM_RIGHT_PAREN},
	"[": {SYMBOL, SYM_LEFT_BRACK},
	"]": {SYMBOL, SYM_RIGHT_BRACK},
	".": {SYMBOL, SYM_PERIOD},
	",": {SYMBOL, SYM_COMMA},
	";": {SYMBOL, SYM_SEMICOLON},
	"+": {SYMBOL, SYM_PLUS},
	"-": {SYMBOL, SYM_MINUS},
	"*": {SYMBOL, SYM_ASTERISK},
	"/": {SYMBOL, SYM_SLASH},
	"&": {SYMBOL, SYM_AMPERSAND},
	"|": {SYMBOL, SYM_PIPE},
	"<": {SYMBOL, SYM_LESS_THAN},
	">": {SYMBOL, SYM_GREATER_THAN},
	"=": {SYMBOL, SYM_EQUALS},
	"~": {SYMBOL, SYM_TILDE},
}
./tokenizer/tokenizer.go
package jack_tokenizer

import (
	"errors"
	"io"
	"strconv"
	"strings"
)

type scanner struct {
	bytes []byte
	index int
}

func (s *scanner) atEnd() bool {
	return s.index >= len(s.bytes)
}

func (s *scanner) advance() {
	s.index++
}

func (s *scanner) peek() (byte, error) {
	if s.atEnd() || s.index+1 >= len(s.bytes) {
		return 0, errors.New("peek beyond")
	}

	return s.bytes[s.index+1], nil
}

func (s *scanner) current() byte {
	return s.bytes[s.index]
}

func NewScanner(b []byte) scanner {
	return scanner{b, 0}
}

func isNumber(b byte) bool {
	_, err := strconv.ParseInt(string(b), 10, 8)
	return err == nil
}

func isLetter(c byte) bool {
	return (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')
}

func Tokenize(r io.Reader) ([]Token, error) {
	chars, err := io.ReadAll(r)
	if err != nil {
		return nil, err
	}
	var parseError error
	parseError = nil
	scan := NewScanner(chars)
	tokens := make([]Token, 0)
	line := 0

	writeInt := func() (string, error) {
		var ss strings.Builder
		for !scan.atEnd() && isNumber(scan.current()) {
			ss.WriteByte(scan.current())
			scan.advance()
		}

		if !scan.atEnd() && isLetter(scan.current()) {
			return "", errors.New("letter found " + string(scan.current()))
		}

		return ss.String(), nil
	}

	for !scan.atEnd() {
		ch := scan.current()
		pair, ok := mp[string(ch)]
		switch {
		// symbols
		case ch == '/':
			peek, err := scan.peek()
			if err != nil {
				parseError = err
			} else {
				if peek == '/' { // single comment
					for !scan.atEnd() && scan.current() != '\n' {
						scan.advance()
					}
				} else if peek == '*' { // multi line comment
					scan.advance()
					peek, _ = scan.peek()
					for !scan.atEnd() && (scan.current() != '*' || peek != '/') {
						scan.advance()
						peek, _ = scan.peek()
					}
					scan.advance()
				} else {
					tokens = append(tokens, NewToken(string(ch), line, SYMBOL, SYM_SLASH))
				}
			}
			scan.advance()
		case ch == '"':
			var ss strings.Builder
			scan.advance()
			for !scan.atEnd() && (scan.current() != '\n' && scan.current() != '"') {
				ss.WriteByte(scan.current())
				scan.advance()
			}

			sres := ss.String()
			if !scan.atEnd() && scan.current() == '"' {
				tokens = append(tokens, NewToken(sres, line, STRING_CONSTANT, NONE))
				scan.advance()
			} else {
				parseError = errors.New("unterminated string")
				tokens = append(tokens, NewToken(sres, line, ERROR, NONE))
			}
		case ok && pair.tt == SYMBOL:
			tokens = append(tokens, NewToken(string(ch), line, pair.tt, pair.st))
			scan.advance()
		case isNumber(ch):
			tt := INT_CONSTANT
			st := NONE
			numberStr, err := writeInt()
			if err != nil {
				tt = ERROR
				parseError = err
			}
			tokens = append(tokens, NewToken(numberStr, line, TokenType(tt), TokenSubtype(st)))

		case isLetter(ch) || ch == '_': // identifier, or keyword
			var ss strings.Builder
			for !scan.atEnd() && isLetter(scan.current()) {
				ss.WriteByte(scan.current())
				scan.advance()
			}
			sres := ss.String()
			pair, ok = mp[sres]
			if ok { // keyword
				tokens = append(tokens, NewToken(sres, line, pair.tt, pair.st))
			} else { // identifier
				tokens = append(tokens, NewToken(sres, line, IDENTIFIER, NONE))
			}
		default: // whitespace or unrecognized
			scan.advance()
		}

		// scan.advance()
		line++
	}

	return tokens, parseError
}
./tokenizer/tokensubtype_string.go
// Code generated by "stringer -type=TokenSubtype"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[UNKNOWN - -1]
	_ = x[NONE-1]
	_ = x[KW_CLASS-2]
	_ = x[KW_CONSTRUCTOR-3]
	_ = x[KW_FUNCTION-4]
	_ = x[KW_METHOD-5]
	_ = x[KW_FIELD-6]
	_ = x[KW_STATIC-7]
	_ = x[KW_VAR-8]
	_ = x[KW_INT-9]
	_ = x[KW_CHAR-10]
	_ = x[KW_BOOLEAN-11]
	_ = x[KW_VOID-12]
	_ = x[KW_TRUE-13]
	_ = x[KW_FALSE-14]
	_ = x[KW_NULL-15]
	_ = x[KW_THIS-16]
	_ = x[KW_LET-17]
	_ = x[KW_DO-18]
	_ = x[KW_IF-19]
	_ = x[KW_ELSE-20]
	_ = x[KW_WHILE-21]
	_ = x[KW_RETURN-22]
	_ = x[SYM_LEFT_BRACE-23]
	_ = x[SYM_RIGHT_BRACE-24]
	_ = x[SYM_LEFT_PAREN-25]
	_ = x[SYM_RIGHT_PAREN-26]
	_ = x[SYM_LEFT_BRACK-27]
	_ = x[SYM_RIGHT_BRACK-28]
	_ = x[SYM_PERIOD-29]
	_ = x[SYM_COMMA-30]
	_ = x[SYM_SEMICOLON-31]
	_ = x[SYM_PLUS-32]
	_ = x[SYM_MINUS-33]
	_ = x[SYM_ASTERISK-34]
	_ = x[SYM_SLASH-35]
	_ = x[SYM_AMPERSAND-36]
	_ = x[SYM_PIPE-37]
	_ = x[SYM_LESS_THAN-38]
	_ = x[SYM_GREATER_THAN-39]
	_ = x[SYM_EQUALS-40]
	_ = x[SYM_TILDE-41]
}

const (
	_TokenSubtype_name_0 = "UNKNOWN"
	_TokenSubtype_name_1 = "NONEKW_CLASSKW_CONSTRUCTORKW_FUNCTIONKW_METHODKW_FIELDKW_STATICKW_VARKW_INTKW_CHARKW_BOOLEANKW_VOIDKW_TRUEKW_FALSEKW_NULLKW_THISKW_LETKW_DOKW_IFKW_ELSEKW_WHILEKW_RETURNSYM_LEFT_BRACESYM_RIGHT_BRACESYM_LEFT_PARENSYM_RIGHT_PARENSYM_LEFT_BRACKSYM_RIGHT_BRACKSYM_PERIODSYM_COMMASYM_SEMICOLONSYM_PLUSSYM_MINUSSYM_ASTERISKSYM_SLASHSYM_AMPERSANDSYM_PIPESYM_LESS_THANSYM_GREATER_THANSYM_EQUALSSYM_TILDE"
)

var (
	_TokenSubtype_index_1 = [...]uint16{0, 4, 12, 26, 37, 46, 54, 63, 69, 75, 82, 92, 99, 106, 114, 121, 128, 134, 139, 144, 151, 159, 168, 182, 197, 211, 226, 240, 255, 265, 274, 287, 295, 304, 316, 325, 338, 346, 359, 375, 385, 394}
)

func (i TokenSubtype) String() string {
	switch {
	case i == -1:
		return _TokenSubtype_name_0
	case 1 <= i && i <= 41:
		i -= 1
		return _TokenSubtype_name_1[_TokenSubtype_index_1[i]:_TokenSubtype_index_1[i+1]]
	default:
		return "TokenSubtype(" + strconv.FormatInt(int64(i), 10) + ")"
	}
}
./tokenizer/tokentype_string.go
// Code generated by "stringer -type=TokenType"; DO NOT EDIT.

package jack_tokenizer

import "strconv"

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[KEYWORD-0]
	_ = x[SYMBOL-1]
	_ = x[INT_CONSTANT-2]
	_ = x[STRING_CONSTANT-3]
	_ = x[IDENTIFIER-4]
}

const _TokenType_name = "KEYWORDSYMBOLINT_CONSTANTSTRING_CONSTANTIDENTIFIER"

var _TokenType_index = [...]uint8{0, 7, 13, 25, 40, 50}

func (i TokenType) String() string {
	if i < 0 || i >= TokenType(len(_TokenType_index)-1) {
		return "TokenType(" + strconv.FormatInt(int64(i), 10) + ")"
	}
	return _TokenType_name[_TokenType_index[i]:_TokenType_index[i+1]]
}
./tokenwriter/writer.go
package jack_tokenwriter

import (
	"fmt"
	"io"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func CreateXMLWriter(tokens []jack_tokenizer.Token) func(io.Writer) {
	var ss strings.Builder

	return func(w io.Writer) {
		for _, token := range tokens {
			tagname, ok := keyword2tag[token.Tokentype]
			if !ok {
				tagname = "unknown"
			}
			escaped, ok := escapeLexeme[token.Lexeme]
			if !ok {
				escaped = token.Lexeme
			}
			ss.WriteString(wrap(tagname, escaped, 1))
		}
		var root strings.Builder
		root.WriteString(wrap("tokens", "\n"+ss.String(), 0))

		io.WriteString(w, root.String())
	}
}

func wrap(tagName, content string, tabs int) string {
	return fmt.Sprintf("%s<%s>%s</%s>\n", strings.Repeat("\t", tabs), tagName, content, tagName)
}

var keyword2tag = map[jack_tokenizer.TokenType]string{
	jack_tokenizer.KEYWORD:         "keyword",
	jack_tokenizer.SYMBOL:          "symbol",
	jack_tokenizer.INT_CONSTANT:    "integerConstant",
	jack_tokenizer.STRING_CONSTANT: "stringConstant",
	jack_tokenizer.IDENTIFIER:      "identifier",
}

var escapeLexeme = map[string]string{
	"<":  "&lt;",
	">":  "&gt;",
	"\"": "&quot;",
	"&":  "&amp;",
}
./tokenwriter/writer_test.go
package jack_tokenwriter

import (
	"fmt"
	"os"
	"strings"
	"testing"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func TestCreateXMLWriter(t *testing.T) {
	tests := []struct {
		name string
	}{
		// TODO: Add test cases
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ArrayTest"},
		// {"/home/jr/school/cs3650/nand2tetris/projects/10/ExpressionLessSquare"},
		{"/home/jr/school/cs3650/nand2tetris/projects/10/Square"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			dirs, err := os.ReadDir(tt.name)
			if err != nil {
				t.Errorf("no such directory: %s", tt.name)
			}

			for _, entry := range dirs {
				if strings.HasSuffix(entry.Name(), ".jack") {
					file, err := os.Open(fmt.Sprintf("%s/%s", tt.name, entry.Name()))
					if err != nil {
						t.Errorf("failed to open file: %s", tt.name)
					}

					tokens, err := jack_tokenizer.Tokenize(file)

					if err != nil {
						t.Errorf("failed to tokenize: %s", err)
					}

					toOut := CreateXMLWriter(tokens)
					fmt.Println("outputting")
					toOut(os.Stdout)
				}
			}
		})
	}
}
./compiler/vmwriter.go
package jack_compiler

import (
	"fmt"
	"io"
)

type VMWriter struct {
	io.WriteCloser
}

var sm = map[SegmentType]string{
	CONSTANT: "constant",
	ARGUMENT: "arg",
	LOCAL:    "local",
	STATIC_S: "static",
	THIS:     "this",
	THAT:     "that",
	POINTER:  "pointer",
	TEMP:     "temp",
}

var am = map[ArithmeticType]string{
	ADD: "add",
	SUB: "sub",
	NEG: "neg",
	EQ:  "eq",
	GT:  "gt",
	LT:  "lt",
	AND: "and",
	OR:  "or",
	NOT: "not",
}

func (s *VMWriter) WritePush(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("push %s %d\n", sm[segment], index))
}

func (s *VMWriter) WritePop(segment SegmentType, index int) {
	io.WriteString(s, fmt.Sprintf("pop %s %d\n", sm[segment], index))
}

func (s *VMWriter) WriteArithmetic(arithmetic ArithmeticType) {
	io.WriteString(s, am[arithmetic]+"\n")
}

func (s *VMWriter) WriteLabel(label string) {
	io.WriteString(s, fmt.Sprintf("label %s\n", label))
}

func (s *VMWriter) WriteGoto(label string) {
	io.WriteString(s, fmt.Sprintf("goto %s\n", label))
}

func (s *VMWriter) WriteIf(label string) {
	io.WriteString(s, fmt.Sprintf("if-goto %s\n", label))
}

func (s *VMWriter) WriteCall(name string, nArgs int) {
	io.WriteString(s, fmt.Sprintf("call %s %d\n", name, nArgs))
}

func (s *VMWriter) WriteFunction(name Name, nvars int) {
	io.WriteString(s, fmt.Sprintf("function %s %d\n", name, nvars))
}

func (s *VMWriter) WriteReturn() {
	io.WriteString(s, "return\n")
}

func NewVMWriter(w io.WriteCloser) *VMWriter {
	return &VMWriter{w}
}
./compiler/symboltable.go
package jack_compiler

import jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"

type Name string

type tableentry struct {
	index  int
	typing string
}

type FieldType int

const (
	STATIC_F FieldType = iota
	FIELD
	ARG
	VAR

	NONE
)

var fieldtoSegment = map[FieldType]SegmentType{
	STATIC_F: STATIC_S,
	FIELD:    THIS,
	VAR:      LOCAL,
	ARG:      ARGUMENT,
}

var constructorTTtoFT = map[jack_tokenizer.TokenSubtype]FieldType{
	jack_tokenizer.KW_STATIC: STATIC_F,
	jack_tokenizer.KW_FIELD:  VAR,
}

var subtypeToOp = map[jack_tokenizer.TokenSubtype]ArithmeticType{
	jack_tokenizer.SYM_PLUS:         ADD,
	jack_tokenizer.SYM_MINUS:        SUB,
	jack_tokenizer.SYM_AMPERSAND:    AND,
	jack_tokenizer.SYM_LESS_THAN:    LT,
	jack_tokenizer.SYM_GREATER_THAN: GT,
	jack_tokenizer.SYM_EQUALS:       EQ,
	jack_tokenizer.SYM_PIPE:         OR,
	jack_tokenizer.SYM_TILDE:        NOT,
}

type SymbolTable struct {
	staticTable map[Name]tableentry
	fieldTable  map[Name]tableentry
	argTable    map[Name]tableentry
	varTable    map[Name]tableentry

	staticIndex int
	fieldIndex  int
	argIndex    int
	varIndex    int
}

func (s *SymbolTable) getTable(kind FieldType) (*map[Name]tableentry, *int) {
	switch kind {
	case STATIC_F:
		return &s.staticTable, &s.staticIndex
	case FIELD:
		return &s.fieldTable, &s.fieldIndex
	case ARG:
		return &s.argTable, &s.argIndex
	case VAR:
		return &s.varTable, &s.varIndex
	default:
		panic("unknown segment")
	}
}

func (sym *SymbolTable) find(n Name) *map[Name]tableentry {
	if _, ok := sym.staticTable[n]; ok {
		return &sym.staticTable
	}

	if _, ok := sym.fieldTable[n]; ok {
		return &sym.fieldTable
	}

	if _, ok := sym.argTable[n]; ok {
		return &sym.argTable
	}

	if _, ok := sym.varTable[n]; ok {
		return &sym.varTable
	}

	return nil
}

func (sym *SymbolTable) Reset() {
	sym.staticTable = make(map[Name]tableentry)
	sym.fieldTable = make(map[Name]tableentry)
	sym.argTable = make(map[Name]tableentry)
	sym.varTable = make(map[Name]tableentry)

	sym.staticIndex = 0
	sym.fieldIndex = 0
	sym.argIndex = 0
	sym.varIndex = 0
}

func (sym *SymbolTable) Define(n Name, t string, kind FieldType) {
	table, index := sym.getTable(kind)

	(*table)[n] = tableentry{*index, t}
	*index += 1
}

func (sym *SymbolTable) VarCount(kind FieldType) int {
	table, _ := sym.getTable(kind)

	return len(*table)
}

func (sym *SymbolTable) KindOf(n Name) FieldType {
	if _, ok := sym.staticTable[n]; ok {
		return STATIC_F
	}

	if _, ok := sym.fieldTable[n]; ok {
		return FIELD
	}

	if _, ok := sym.argTable[n]; ok {
		return ARG
	}

	if _, ok := sym.varTable[n]; ok {
		return VAR
	}

	return NONE
}

func (sym *SymbolTable) TypeOf(n Name) string {
	table := sym.find(n)

	return (*table)[n].typing
}

func (sym *SymbolTable) IndexOf(n Name) int {
	table := sym.find(n)

	return (*table)[n].index
}

func NewSymbolTable() *SymbolTable {
	return &SymbolTable{}
}

type SegmentType int

const (
	CONSTANT SegmentType = iota
	ARGUMENT
	LOCAL
	STATIC_S
	THIS
	THAT
	POINTER
	TEMP
)

type ArithmeticType int

const (
	ADD ArithmeticType = iota
	SUB
	NEG
	EQ
	GT
	LT
	AND
	OR
	NOT
)
./compiler/compiler.go
package jack_compiler

import (
	"errors"
	"fmt"
	"io"
	"strconv"
	"strings"

	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

type tokenpair struct {
	tt jack_tokenizer.TokenType
	st jack_tokenizer.TokenSubtype
}

var typePair = []tokenpair{
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_INT},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CHAR},
	{jack_tokenizer.KEYWORD, jack_tokenizer.KW_BOOLEAN},
	{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
}

type parser struct {
	tokens []jack_tokenizer.Token
	index  int

	err          error
	subroutineSt *SymbolTable
	classSt      *SymbolTable
	vmWriter     VMWriter

	className   string
	labelNumber int
}

type symboldata struct {
	name   string
	symbol FieldType
	index  int
}

func NewParser(tokens []jack_tokenizer.Token, vmw VMWriter) *parser {
	return &parser{
		tokens,
		0,
		nil,
		NewSymbolTable(),
		NewSymbolTable(),
		vmw,
		"",
		0,
	}
}

func (s *parser) resolveSymbol(sym string) symboldata {
	res := s.subroutineSt.KindOf(Name(sym))

	if res == NONE {
		res = s.classSt.KindOf(Name(sym))
		if res == NONE {
			return symboldata{
				sym,
				NONE,
				0,
			}
		}

		return symboldata{
			sym,
			res,
			s.classSt.IndexOf(Name(sym)),
		}
	}

	return symboldata{
		sym,
		res,
		s.subroutineSt.IndexOf(Name(sym)),
	}
}

func (s *parser) process(pairs []tokenpair) (*jack_tokenizer.Token, error) {
	var err error
	if !s.matches(pairs) {
		var ss strings.Builder
		for _, p := range pairs {
			ss.WriteString(fmt.Sprintf("{ %s, %s }", p.tt.String(), p.st.String()))
		}

		s.err = fmt.Errorf("%s %s %s %s: grammar error: got %s, wanted %s @ %d", s.tokens[s.index-2].Lexeme, s.tokens[s.index-1].Lexeme, s.tokens[s.index].Lexeme, s.tokens[s.index+1].Lexeme, s.Current().Lexeme, ss.String(), s.index)
		err = s.err
	}
	ret := s.Current()
	s.Advance()

	return ret, err
}

func (s *parser) matches(pairs []tokenpair) bool {
	curr := s.Current()
	for _, p := range pairs {
		if p.tt == curr.Tokentype && p.st == curr.Subtype {
			return true
		}
	}

	return false
}

func (s *parser) atEnd() bool {
	return s.index >= len(s.tokens)
}

func (s *parser) peek() (*jack_tokenizer.Token, error) {
	if s.atEnd() || s.index+1 >= len(s.tokens) {
		return nil, errors.New("error at end")
	}

	return &s.tokens[s.index+1], nil
}

func (s *parser) Current() *jack_tokenizer.Token {
	return &s.tokens[s.index]
}

func (s *parser) Advance() {
	s.index++
}

func (s *parser) Parse() error {
	s.Class()

	return s.err
}

func (s *parser) helper_type(additional []tokenpair) (*jack_tokenizer.Token, error) {
	tp := make([]tokenpair, 0)
	tp = append(tp, typePair...)
	tp = append(tp, additional...)

	return s.process(tp)
}

func (s *parser) symbolHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, st},
	})
}

func (s *parser) identifierHelper() (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
	})
}

func (s *parser) keywordHelper(st jack_tokenizer.TokenSubtype) (*jack_tokenizer.Token, error) {
	return s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, st},
	})
}

// compiles a Class
func (s *parser) Class() {
	s.classSt.Reset()
	s.subroutineSt.Reset()

	s.keywordHelper(jack_tokenizer.KW_CLASS)
	token, err := s.identifierHelper()
	if err == nil {
		s.className = token.Lexeme
	}
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	}) {
		s.ClassVarDec()
	}

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
	}) {
		s.Subroutine()
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
}

// Compiles a static variable declaration or a field declaration
func (s *parser) ClassVarDec() {
	var ft FieldType
	var typing string
	var varName string
	hasError := false
	token, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_STATIC},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FIELD},
	})

	if err == nil {
		ft = constructorTTtoFT[token.Subtype]
	} else {
		hasError = true
	}

	token, err = s.helper_type([]tokenpair{})

	if err == nil {
		typing = token.Lexeme
	} else {
		hasError = true
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	} else {
		hasError = true
	}

	if !hasError {
		s.classSt.Define(Name(varName), typing, ft)
	}

	// (',' varName)
	for s.matches([]tokenpair{{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA}}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
			s.classSt.Define(Name(varName), typing, ft)
		}
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a complete method, function or constructor
func (s *parser) Subroutine() {
	s.subroutineSt.Reset()

	keyToken, err := s.process([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_CONSTRUCTOR},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FUNCTION},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_METHOD},
	})

	if err == nil && keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.subroutineSt.Define("this", s.className, ARG)
	}

	s.helper_type([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VOID},
	})

	token, _ := s.identifierHelper()
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	nargs := s.Parameters()

	s.vmWriter.WriteFunction(Name(fmt.Sprintf("%s.%s", s.className, token.Lexeme)), nargs)

	if keyToken.Subtype == jack_tokenizer.KW_METHOD {
		s.vmWriter.WritePush(ARGUMENT, 0)
		s.vmWriter.WritePop(POINTER, 0)
	} else if keyToken.Subtype == jack_tokenizer.KW_CONSTRUCTOR {
		n := s.subroutineSt.VarCount(FIELD)
		s.vmWriter.WritePush(CONSTANT, n)
		s.vmWriter.WriteCall("Memory.alloc", 1)
		s.vmWriter.WritePop(POINTER, 0)
	}

	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.SubroutineBody()
}

// Compiles a (possibly empty) Parameters
// list. Does not handle the enclosing
// parantheses tokens (ands).
func (s *parser) Parameters() int {
	count := 0
	processTypeVarName := func() {
		var typing string
		var varName string
		token, err := s.process(typePair)
		if err == nil {
			typing = token.Lexeme
		}
		token, err = s.identifierHelper()

		if err == nil {
			varName = token.Lexeme
		}

		s.subroutineSt.Define(Name(varName), typing, ARG)
		count++
	}
	if s.matches(typePair) {
		processTypeVarName()

		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			processTypeVarName()
		}
	}

	return count
}

// Compiles a subroutine's body
func (s *parser) SubroutineBody() {
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	for s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_VAR},
	}) {
		s.VarDec()
	}
	s.Statements()
}

// Compiles a var declaration
func (s *parser) VarDec() {
	s.keywordHelper(jack_tokenizer.KW_VAR)
	var typing string
	var varName string

	token, err := s.helper_type(nil)
	if err == nil {
		typing = token.Lexeme
	}

	token, err = s.identifierHelper()

	if err == nil {
		varName = token.Lexeme
	}

	s.subroutineSt.Define(Name(varName), typing, FieldType(VAR))
	for s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_COMMA)
		s.identifierHelper()
	}

	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a sequeneces of statemnents
// Does not handle the enclosing curly
// bracket tokens { and }.
func (s *parser) Statements() {
	states := []tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_LET},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_IF},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_WHILE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_DO},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_RETURN},
	}

	if s.matches(states) {
		for s.matches(states) {
			switch {
			case s.matches([]tokenpair{states[0]}):
				s.LetStatement()
			case s.matches([]tokenpair{states[1]}):
				s.IfStatement()
			case s.matches([]tokenpair{states[2]}):
				s.While()
			case s.matches([]tokenpair{states[3]}):
				s.Do()
			case s.matches([]tokenpair{states[4]}):
				s.ReturnStatement()
			}
		}
	} else {
		s.err = errors.New("unexpected lexeme " + s.Current().Lexeme)
	}
}

// Compiles a let statement.
func (s *parser) LetStatement() {
	s.keywordHelper(jack_tokenizer.KW_LET)
	token, _ := s.identifierHelper()

	if s.matches([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_BRACK},
	}) {
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)

		s.Expression()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
	}

	s.symbolHelper(jack_tokenizer.SYM_EQUALS)

	s.Expression()
	// pop symbolArgName index

	res := s.resolveSymbol(token.Lexeme)
	s.vmWriter.WritePop(fieldtoSegment[res.symbol], res.index)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an if statement
// possibly with a trailing else clause
func (s *parser) IfStatement() {
	s.keywordHelper(jack_tokenizer.KW_IF)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	// if-goto label1
	s.vmWriter.WriteIf(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.labelNumber++
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)

	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

	s.Statements()
	// goto label2
	s.vmWriter.WriteGoto(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)

	if s.matches([]tokenpair{
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_ELSE},
	}) {
		// label l1
		s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber-1))
		s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)

		s.Statements()

		s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	}
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s.IF-%d", s.className, s.labelNumber))
}

// Compiles a While statement
func (s *parser) While() {
	s.keywordHelper(jack_tokenizer.KW_WHILE)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.Expression()
	// not
	s.vmWriter.WriteArithmetic(NOT)
	s.labelNumber++
	// if-goto l2
	s.vmWriter.WriteIf(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
	s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACE)
	s.Statements()
	// goto l1
	s.vmWriter.WriteGoto(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber-1))
	s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACE)
	// label l2
	s.vmWriter.WriteLabel(fmt.Sprintf("%s-IF-%d", s.className, s.labelNumber))
}

// Compiles a Do statement
func (s *parser) Do() {
	s.keywordHelper(jack_tokenizer.KW_DO)
	s.Expression()
	// pop something 0
	s.vmWriter.WritePop(TEMP, 0)
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles a return statement
func (s *parser) ReturnStatement() {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}

	s.keywordHelper(jack_tokenizer.KW_RETURN)
	if s.matches(begTerm) {
		s.Expression()
	}
	// return
	s.vmWriter.WriteReturn()
	s.symbolHelper(jack_tokenizer.SYM_SEMICOLON)
}

// Compiles an Expression
func (s *parser) Expression() {
	op := []tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_SLASH},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_ASTERISK},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PIPE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LESS_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_GREATER_THAN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_EQUALS},
	}
	s.Term()

	for s.matches(op) {
		token, _ := s.process(op)
		s.Term()

		switch token.Subtype {
		case jack_tokenizer.SYM_SLASH:
			s.vmWriter.WriteCall("Math.divide", 2)
		case jack_tokenizer.SYM_ASTERISK:
			s.vmWriter.WriteCall("Math.multiply", 2)
		default:
			s.vmWriter.WriteArithmetic(subtypeToOp[token.Subtype])
		}

	}
}

// Compiles a Term. If the current token is an
// identifier, the routine must resolve it
// into a variable, an array element, or a
// subroutine call. A single lookahead tokezn,
// which may be [, (, or ., suffices to distinguish
// between the possibilities.
// Any other token is not part of this Term
// and should not be advanced over.
func (s *parser) Term() {
	switch s.Current().Tokentype {
	case jack_tokenizer.IDENTIFIER:
		// variable, array element or subroutine
		peek, err := s.peek()
		if err != nil {
			s.err = nil
		} else {
			switch peek.Subtype {
			case jack_tokenizer.SYM_LEFT_PAREN:
			case jack_tokenizer.SYM_PERIOD:
				s.SubroutineCall()
			case jack_tokenizer.SYM_LEFT_BRACK:
				// varname[expression]
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)
				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
				s.symbolHelper(jack_tokenizer.SYM_LEFT_BRACK)
				s.Expression()
				s.vmWriter.WriteArithmetic(ADD)
				s.symbolHelper(jack_tokenizer.SYM_RIGHT_BRACK)
			default:
				token, _ := s.identifierHelper()
				resolved := s.resolveSymbol(token.Lexeme)

				s.vmWriter.WritePush(fieldtoSegment[resolved.symbol], resolved.index)
			}

		}
	case jack_tokenizer.INT_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		})
		i, _ := strconv.Atoi(token.Lexeme)
		s.vmWriter.WritePush(CONSTANT, i)
	case jack_tokenizer.STRING_CONSTANT:
		token, _ := s.process([]tokenpair{
			{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		})
		s.vmWriter.WritePush(CONSTANT, len(token.Lexeme))
		s.vmWriter.WriteCall("String.new", 1)
		for _, c := range []byte(token.Lexeme) {
			s.vmWriter.WritePush(CONSTANT, int(c))
			s.vmWriter.WriteCall("String.appendChar", 1)
		}
		// push c
	case jack_tokenizer.SYMBOL:
		switch s.Current().Subtype {
		case jack_tokenizer.SYM_LEFT_PAREN:
			s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)
			s.Expression()
			s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		case jack_tokenizer.SYM_MINUS:
		case jack_tokenizer.SYM_TILDE:
			s.symbolHelper(s.Current().Subtype)
			s.Term()
			// output op
			tokenName := SUB
			if s.Current().Subtype == jack_tokenizer.SYM_TILDE {
				tokenName = NOT
			}
			s.vmWriter.WriteArithmetic(tokenName)
		}
	case jack_tokenizer.KEYWORD:
		switch s.Current().Subtype {
		case jack_tokenizer.KW_FALSE:
		case jack_tokenizer.KW_NULL:
			s.vmWriter.WritePush(CONSTANT, 0)
		case jack_tokenizer.KW_TRUE:
			s.vmWriter.WritePush(CONSTANT, 1)
			s.vmWriter.WriteArithmetic(NEG)
		case jack_tokenizer.KW_THIS:
			s.vmWriter.WritePush(POINTER, 0)
		}
		s.keywordHelper(s.Current().Subtype)
	}

}

func (s *parser) SubroutineCall() {
	mainToken, _ := s.identifierHelper() // class's name or subroutine name, depending on if theres a .
	mainName := mainToken.Lexeme
	secondaryName := ""

	t := s.Current()
	s.process([]tokenpair{
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PERIOD},
	})

	switch t.Subtype {
	case jack_tokenizer.SYM_PERIOD:
		fn, _ := s.identifierHelper()
		secondaryName = "." + fn.Lexeme
		s.symbolHelper(jack_tokenizer.SYM_LEFT_PAREN)

		fallthrough
	case jack_tokenizer.SYM_LEFT_PAREN:
		n := s.ExpressionList()
		s.symbolHelper(jack_tokenizer.SYM_RIGHT_PAREN)
		s.vmWriter.WriteCall(fmt.Sprintf("%s%s", mainName, secondaryName), n)
	}
}

// Compiles a (possibly empty) comma-
// separated list of expression. Returns
// the number of expressions in the list
func (s *parser) ExpressionList() int {
	begTerm := []tokenpair{
		{jack_tokenizer.STRING_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.INT_CONSTANT, jack_tokenizer.NONE},
		{jack_tokenizer.IDENTIFIER, jack_tokenizer.NONE},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_PLUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_LEFT_PAREN},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_MINUS},
		{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_TILDE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_TRUE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_FALSE},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_NULL},
		{jack_tokenizer.KEYWORD, jack_tokenizer.KW_THIS},
	}
	count := 0
	if s.matches(begTerm) {
		count++
		s.Expression()
		for s.matches([]tokenpair{
			{jack_tokenizer.SYMBOL, jack_tokenizer.SYM_COMMA},
		}) {
			count++
			s.symbolHelper(jack_tokenizer.SYM_COMMA)
			s.Expression()
		}
	}

	return count
}

// x* : 0 or more
// ? one or more
// x y x followed by y
// x | y x or y

func ParseGrammar(tokens []jack_tokenizer.Token) func(io.WriteCloser) error {
	return func(w io.WriteCloser) error {
		parser := NewParser(tokens, *NewVMWriter(w))

		err := parser.Parse()
		return err
	}
}
./main.go
package main

import (
	"fmt"
	"os"
	"strings"

	jack_compiler "github.com/renojcpp/n2t-compiler/compiler"
	jack_tokenizer "github.com/renojcpp/n2t-compiler/tokenizer"
)

func main() {
	args := os.Args[1:]

	for _, arg := range args {
		dirs, err := os.ReadDir(arg)
		if err != nil {
			fmt.Printf("no such directory: %s", arg)
		}

		for _, entry := range dirs {
			if strings.HasSuffix(entry.Name(), ".jack") {
				file, err := os.Open(fmt.Sprintf("%s/%s", arg, entry.Name()))
				if err != nil {
					fmt.Printf("failed to open file: %s", arg)
				}

				tokens, err := jack_tokenizer.Tokenize(file)

				if err != nil {
					fmt.Printf("failed to tokenize: %s", err)
				}

				toOut := jack_compiler.ParseGrammar(tokens)
				fmt.Println("outputting")
				f, _ := os.Create(fmt.Sprintf("%s/%s", arg, entry.Name()+".vm"))
				err = toOut(f)

				if err != nil {
					fmt.Printf("%s", err.Error())
				}
			}
		}
	}
}
